{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Quickstart \n","\n","Data: 1782 parallel and related timeseries \n","   - Sales data for 33 product categories in 54 stores.\n","\n","\n","Use **Darts** for Time Series Forecasting"]},{"cell_type":"markdown","metadata":{},"source":["# 1.Libraries"]},{"cell_type":"markdown","metadata":{},"source":["I use the [**Darts library**](https://unit8co.github.io/darts/index.html) for all my modeling here. It is an excellent and intuitive option for forecasting in Python, especially with regards to NN models. The developing team is very helpful and answering questions in their public communication channels, so I can highly recommend the library for timeseries forecasting!"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-17T01:14:21.69721Z","iopub.status.busy":"2022-11-17T01:14:21.696415Z","iopub.status.idle":"2022-11-17T01:15:22.039168Z","shell.execute_reply":"2022-11-17T01:15:22.037994Z","shell.execute_reply.started":"2022-11-17T01:14:21.697136Z"},"trusted":true},"outputs":[],"source":["# Install libraries\n","\n","#!pip install torch\n","# https://download.pytorch.org/whl/cu111/torch_stable.html\n","\n","# !pip install pyyaml -U\n","# !pip install darts -U\n","# import darts\n","# print(darts.__version__)\n","\n","# !pip install optuna -U\n","# %pip install --upgrade numpy\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","The StatsForecast module could not be imported. To enable support for the StatsForecastAutoARIMA, StatsForecastAutoETS and Croston models, please consider installing it.\n"]}],"source":["import numpy as np\n","import time\n","from pathlib import Path\n","\n","from darts import TimeSeries\n","from darts.utils.timeseries_generation import gaussian_timeseries, linear_timeseries, sine_timeseries\n","from darts.models import LightGBMModel, CatBoostModel, Prophet, RNNModel, TFTModel, NaiveSeasonal, ExponentialSmoothing, NHiTSModel\n","from darts.metrics import mape, smape, rmse, rmsle\n","from darts.dataprocessing import Pipeline\n","from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\n","from darts.utils.timeseries_generation import datetime_attribute_timeseries\n","from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis, plot_hist\n","from darts.utils.likelihood_models import QuantileRegression\n","from darts.utils.missing_values import fill_missing_values\n","# from darts.models import MovingAverage\n","\n","import optuna\n","from optuna.integration import PyTorchLightningPruningCallback\n","from optuna.visualization import (\n","    plot_optimization_history,\n","    plot_contour,\n","    plot_param_importances,\n",")\n","\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","from tqdm import tqdm\n","\n","import sklearn\n","from sklearn import preprocessing\n","\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","import gc\n","\n","%matplotlib inline\n","torch.manual_seed(1); np.random.seed(1)  # for reproducibility"]},{"cell_type":"markdown","metadata":{},"source":["# 2.Data"]},{"cell_type":"markdown","metadata":{},"source":["# 2.1. Pre-Processing"]},{"cell_type":"markdown","metadata":{},"source":["After loading the data, I create **Darts-specific TimeSeries objects**. For the sales data, I generate so-called static covariates for each series (store number, product family, city, region, type and cluster). Those might be used by some models in Darts later on. I also create a set of time-based covariates like weekday, month and year. All covariate series get stacked together.\n","\n","Furthermore, I scale all series between 0 and 1, and then apply a logarithmic transformation to them. I took this idea from [this helpful notebook](https://www.kaggle.com/code/carlmcbrideellis/store-sales-using-the-average-of-the-last-16-days). "]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-11-17T01:15:22.042547Z","iopub.status.busy":"2022-11-17T01:15:22.041607Z","iopub.status.idle":"2022-11-17T01:21:46.49557Z","shell.execute_reply":"2022-11-17T01:21:46.494364Z","shell.execute_reply.started":"2022-11-17T01:15:22.042514Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["skipping\n"]}],"source":["%%script echo skipping\n","\n","# Kaggle data folder\n","\n","# Load all Datasets\n","df_train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv')\n","df_test = pd.read_csv('../input/store-sales-time-series-forecasting/test.csv')\n","df_holidays_events = pd.read_csv('../input/store-sales-time-series-forecasting/holidays_events.csv')\n","df_oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv')\n","df_stores = pd.read_csv('../input/store-sales-time-series-forecasting/stores.csv')\n","df_transactions = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv')\n","df_sample_submission = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv')\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/mbp14/Desktop/GoogleDrive/Kaggle/Store_sales_favorita/data/raw\n","/Users/mbp14/Desktop/GoogleDrive/Kaggle/Store_sales_favorita/data/clean\n"]}],"source":["# Local data folder\n","\n","# local data path using pathlib\n","basepath = Path.cwd()\n","datapath = basepath.parent / 'data'\n","\n","# raw data path\n","raw_datapath = datapath / 'raw'\n","\n","# clearn data path\n","clean_datapath = datapath / 'clean'\n","\n","print(raw_datapath)\n","print(clean_datapath)\n","\n","# Load all Datasets\n","df_train = pd.read_csv(raw_datapath/'train.csv')\n","df_test = pd.read_csv(raw_datapath/'test.csv')\n","df_holidays_events = pd.read_csv(raw_datapath/'holidays_events.csv')\n","df_oil = pd.read_csv(raw_datapath/'oil.csv')\n","df_stores = pd.read_csv(raw_datapath/'stores.csv')\n","df_transactions = pd.read_csv(raw_datapath/'transactions.csv')\n","df_sample_submission = pd.read_csv(raw_datapath/'sample_submission.csv')\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Sales Data (Target)\n","\n","family_list = df_train['family'].unique()\n","family_list\n","\n","store_list = df_stores['store_nbr'].unique()\n","store_list\n","\n","\n","train_merged = pd.merge(df_train, df_stores, on ='store_nbr')\n","train_merged = train_merged.sort_values([\"store_nbr\",\"family\",\"date\"])\n","train_merged = train_merged.astype({\"store_nbr\":'str', \"family\":'str', \"city\":'str',\n","                          \"state\":'str', \"type\":'str', \"cluster\":'str'})\n","\n","\n","df_test_sorted = df_test.sort_values(by=['store_nbr','family'])\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["## Clustered by Product Family: Create TimeSeries objects and arrange in a Dictionary\n","\n","family_TS_dict = {}\n","\n","for family in family_list:\n","  df_family = train_merged.loc[train_merged['family'] == family]\n","\n","  list_of_TS_family = TimeSeries.from_group_dataframe(\n","                                df_family,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n","                                value_cols=\"sales\", # target variable\n","                                fill_missing_dates=True,\n","                                freq='D')\n","  for ts in list_of_TS_family:\n","            ts = ts.astype(np.float32)\n","\n","  list_of_TS_family = sorted(list_of_TS_family, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n","  family_TS_dict[family] = list_of_TS_family\n","\n","\n","# Transform the Sales Data\n","family_pipeline_dict = {}\n","family_TS_transformed_dict = {}\n","\n","for key in family_TS_dict:\n","  train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","  static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n","  log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","  train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","  train_pipeline = Pipeline([train_filler,\n","                             static_cov_transformer,\n","                             log_transformer,\n","                             train_scaler])\n","     \n","  training_transformed = train_pipeline.fit_transform(family_TS_dict[key])\n","  family_pipeline_dict[key] = train_pipeline\n","  family_TS_transformed_dict[key] = training_transformed\n","\n","# Create TimeSeries objects (Darts) 1782\n","\n","list_of_TS = TimeSeries.from_group_dataframe(\n","            train_merged,\n","            time_col=\"date\",\n","            group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","            static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n","            value_cols=\"sales\", # target variable\n","            fill_missing_dates=True,\n","            freq='D')\n","\n","for ts in list_of_TS:\n","            ts = ts.astype(np.float32)\n","\n","list_of_TS = sorted(list_of_TS, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n","\n","# Transform the Sales Data\n","\n","train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n","log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_pipeline = Pipeline([train_filler,\n","                             static_cov_transformer,\n","                             log_transformer,\n","                             train_scaler])\n","     \n","training_transformed = train_pipeline.fit_transform(list_of_TS)\n","\n","#train_merged.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Create 7-day and 28-day moving average of sales\n","\n","sales_moving_average_7 = MovingAverage(window=7)\n","sales_moving_average_28 = MovingAverage(window=28)\n","\n","sales_moving_averages_dict = {}\n","\n","for key in family_TS_transformed_dict:\n","  sales_mas_family = []\n","  \n","  for ts in family_TS_transformed_dict[key]:\n","    ma_7 = sales_moving_average_7.filter(ts)\n","    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n","    ma_7 = ma_7.astype(np.float32)\n","    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"sales_ma_7\")\n","    ma_28 = sales_moving_average_28.filter(ts)\n","    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n","    ma_28 = ma_28.astype(np.float32)\n","    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"sales_ma_28\")\n","    mas = ma_7.stack(ma_28)\n","    sales_mas_family.append(mas)\n","  \n","  sales_moving_averages_dict[key] = sales_mas_family  \n","    \n","# General Covariates (Time-Based and Oil)\n","\n","full_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n","\n","# Time-Based Covariates\n","\n","year = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\n","month = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\n","day = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\n","dayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\n","weekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\n","weekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\n","timesteps = TimeSeries.from_times_and_values(times=full_time_period,\n","                                             values=np.arange(len(full_time_period)),\n","                                             columns=[\"linear_increase\"])\n","\n","time_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\n","time_cov = time_cov.astype(np.float32)\n","\n","# Transform\n","time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","time_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\n","time_cov_scaler.fit(time_cov_train)\n","time_cov_transformed = time_cov_scaler.transform(time_cov)\n","\n","#time_cov_transformed[-50:].plot()\n","\n","# Oil Price\n","\n","oil = TimeSeries.from_dataframe(df_oil, \n","                                time_col = 'date', \n","                                value_cols = ['dcoilwtico'],\n","                                freq = 'D')\n","\n","oil = oil.astype(np.float32)\n","\n","# Transform\n","oil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","oil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","oil_pipeline = Pipeline([oil_filler, oil_scaler])\n","oil_transformed = oil_pipeline.fit_transform(oil)\n","\n","# Moving Averages for Oil Price\n","oil_moving_average_7 = MovingAverage(window=7)\n","oil_moving_average_28 = MovingAverage(window=28)\n","\n","oil_moving_averages = []\n","\n","ma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\n","ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\n","ma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\n","ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\n","oil_moving_averages = ma_7.stack(ma_28)\n","\n","# Stack General Covariates Together\n","\n","general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)\n","\n","# Store-Specific Covariates (Transactions and Holidays)\n","\n","# Transactions\n","df_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\n","\n","TS_transactions_list = TimeSeries.from_group_dataframe(\n","                                df_transactions,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                value_cols=\"transactions\",\n","                                fill_missing_dates=True,\n","                                freq='D')\n","\n","transactions_list = []\n","\n","for ts in TS_transactions_list:\n","            series = TimeSeries.from_series(ts.pd_series())   # necessary workaround to remove static covariates (so I can stack covariates later on)\n","            series = series.astype(np.float32)\n","            transactions_list.append(series)\n","\n","transactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n","\n","from datetime import datetime, timedelta\n","\n","transactions_list_full = []\n","\n","for ts in transactions_list:\n","  if ts.start_time() > pd.Timestamp('20130101'):\n","    end_time = (ts.start_time() - timedelta(days=1))\n","    delta = end_time - pd.Timestamp('20130101')\n","    zero_series = TimeSeries.from_times_and_values(\n","                              times=pd.date_range(start=pd.Timestamp('20130101'), \n","                              end=end_time, freq=\"D\"),\n","                              values=np.zeros(delta.days+1))\n","    ts = zero_series.append(ts)\n","    transactions_list_full.append(ts)\n","\n","transactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","transactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","\n","transactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\n","transactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)\n","\n","# Moving Averages for Transactions\n","trans_moving_average_7 = MovingAverage(window=7)\n","trans_moving_average_28 = MovingAverage(window=28)\n","\n","transactions_covs = []\n","\n","for ts in transactions_transformed:\n","  ma_7 = trans_moving_average_7.filter(ts).astype(np.float32)\n","  ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"transactions_ma_7\")\n","  ma_28 = trans_moving_average_28.filter(ts).astype(np.float32)\n","  ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"transactions_ma_28\")\n","  trans_and_mas = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\").stack(ma_7).stack(ma_28)\n","  transactions_covs.append(trans_and_mas)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Re-Defining Categories of Holidays in a Meaningful Way\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['transferred'] == True,'Transferred', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Transfer','Holiday', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Additional','Holiday', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Bridge','Holiday', \n","                                      df_holidays_events['type'])\n","\n","\n","# Assign Holidays to all TimeSeries and Save in Dictionary\n","\n","def holiday_list(df_stores):\n","\n","    listofseries = []\n","    \n","    for i in range(0,len(df_stores)):\n","            \n","            df_holiday_dummies = pd.DataFrame(columns=['date'])\n","            df_holiday_dummies[\"date\"] = df_holidays_events[\"date\"]\n","            \n","            df_holiday_dummies[\"national_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & (df_holidays_events[\"locale\"] == \"National\")), 1, 0)\n","\n","            df_holiday_dummies[\"earthquake_relief\"] = np.where(df_holidays_events['description'].str.contains('Terremoto Manabi'), 1, 0)\n","\n","            df_holiday_dummies[\"christmas\"] = np.where(df_holidays_events['description'].str.contains('Navidad'), 1, 0)\n","\n","            df_holiday_dummies[\"football_event\"] = np.where(df_holidays_events['description'].str.contains('futbol'), 1, 0)\n","\n","            df_holiday_dummies[\"national_event\"] = np.where(((df_holidays_events[\"type\"] == \"Event\") & (df_holidays_events[\"locale\"] == \"National\") & (~df_holidays_events['description'].str.contains('Terremoto Manabi')) & (~df_holidays_events['description'].str.contains('futbol'))), 1, 0)\n","\n","            df_holiday_dummies[\"work_day\"] = np.where((df_holidays_events[\"type\"] == \"Work Day\"), 1, 0)\n","\n","            df_holiday_dummies[\"local_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & ((df_holidays_events[\"locale_name\"] == df_stores['state'][i]) | (df_holidays_events[\"locale_name\"] == df_stores['city'][i]))), 1, 0)\n","                     \n","            listofseries.append(df_holiday_dummies)\n","\n","    return listofseries\n","\n","def remove_0_and_duplicates(holiday_list):\n","\n","    listofseries = []\n","    \n","    for i in range(0,len(holiday_list)):\n","            \n","            df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n","\n","            df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]\n","            \n","            df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max', \n","                                   'christmas':'max', 'football_event':'max', \n","                                   'national_event':'max', 'work_day':'max', \n","                                   'local_holiday':'max'}).reset_index()\n","\n","            listofseries.append(df_holiday_per_store)\n","\n","    return listofseries\n","\n","def holiday_TS_list_54(holiday_list):\n","\n","    listofseries = []\n","    \n","    for i in range(0,54):\n","            \n","            holidays_TS = TimeSeries.from_dataframe(list_of_holidays_per_store[i], \n","                                        time_col = 'date',\n","                                        fill_missing_dates=True,\n","                                        fillna_value=0,\n","                                        freq='D')\n","            \n","            holidays_TS = holidays_TS.slice(pd.Timestamp('20130101'),pd.Timestamp('20170831'))\n","            holidays_TS = holidays_TS.astype(np.float32)\n","            listofseries.append(holidays_TS)\n","\n","    return listofseries\n","\n","\n","list_of_holidays_per_store = holiday_list(df_stores)\n","list_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)   \n","list_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\n","\n","holidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","holidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","\n","holidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\n","holidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Stack Together Store-Specific Covariates with General Covariates\n","\n","store_covariates_future = []\n","\n","for store in range(0,len(store_list)):\n","  stacked_covariates = holidays_transformed[store].stack(general_covariates)  \n","  store_covariates_future.append(stacked_covariates)\n","\n","store_covariates_past = []\n","holidays_transformed_sliced = holidays_transformed # for slicing past covariates\n","\n","for store in range(0,len(store_list)):\n","  holidays_transformed_sliced[store] = holidays_transformed[store].slice_intersect(transactions_covs[store])\n","  general_covariates_sliced = general_covariates.slice_intersect(transactions_covs[store])\n","  stacked_covariates = transactions_covs[store].stack(holidays_transformed_sliced[store]).stack(general_covariates_sliced)  \n","  store_covariates_past.append(stacked_covariates)\n","    \n","# Store/Family-Varying Covariates (Promotion)\n","\n","df_promotion = pd.concat([df_train, df_test], axis=0)\n","df_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\n","df_promotion.tail()\n","\n","family_promotion_dict = {}\n","\n","for family in family_list:\n","  df_family = df_promotion.loc[df_promotion['family'] == family]\n","\n","  list_of_TS_promo = TimeSeries.from_group_dataframe(\n","                                df_family,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                value_cols=\"onpromotion\", # covariate of interest\n","                                fill_missing_dates=True,\n","                                freq='D')\n","  \n","  for ts in list_of_TS_promo:\n","            ts = ts.astype(np.float32)\n","\n","  family_promotion_dict[family] = list_of_TS_promo\n","\n","promotion_transformed_dict = {}\n","\n","for key in tqdm(family_promotion_dict):\n","  promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","  promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","  promo_pipeline = Pipeline([promo_filler,\n","                             promo_scaler])\n","  \n","  promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n","\n","  # Moving Averages for Promotion Family Dictionaries\n","  promo_moving_average_7 = MovingAverage(window=7)\n","  promo_moving_average_28 = MovingAverage(window=28)\n","\n","  promotion_covs = []\n","\n","  for ts in promotion_transformed:\n","    ma_7 = promo_moving_average_7.filter(ts)\n","    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n","    ma_7 = ma_7.astype(np.float32)\n","    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n","    ma_28 = promo_moving_average_28.filter(ts)\n","    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n","    ma_28 = ma_28.astype(np.float32)\n","    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n","    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n","    promotion_covs.append(promo_and_mas)\n","\n","  promotion_transformed_dict[key] = promotion_covs\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# 2.5. Assemble All Covariates in Dictionaries\n","\n","past_covariates_dict = {}\n","\n","for key in tqdm(promotion_transformed_dict):\n","\n","  promotion_family = promotion_transformed_dict[key]\n","  sales_mas = sales_moving_averages_dict[key]\n","  covariates_past = [promotion_family[i].slice_intersect(store_covariates_past[i]).stack(store_covariates_past[i].stack(sales_mas[i])) for i in range(0,len(promotion_family))]\n","\n","  past_covariates_dict[key] = covariates_past\n","\n","future_covariates_dict = {}\n","\n","for key in tqdm(promotion_transformed_dict):\n","\n","  promotion_family = promotion_transformed_dict[key]\n","  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n","\n","  future_covariates_dict[key] = covariates_future\n","\n","only_past_covariates_dict = {}\n","\n","for key in tqdm(sales_moving_averages_dict):\n","  sales_moving_averages = sales_moving_averages_dict[key]\n","  only_past_covariates = [sales_moving_averages[i].stack(transactions_covs[i]) for i in range(0,len(sales_moving_averages))]\n","\n","  only_past_covariates_dict[key] = only_past_covariates\n","\n","# Delete Original Dataframes to Save Memory\n","\n","del(df_train)\n","del(df_test)\n","del(df_stores)\n","del(df_holidays_events)\n","del(df_oil)\n","del(df_transactions)\n","gc.collect()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%script echo skipping\n","\n","# Sales Data (Target)\n","\n","family_list = df_train['family'].unique()\n","family_list\n","store_list = df_stores['store_nbr'].unique()\n","store_list\n","\n","train_merged = pd.merge(df_train, df_stores, on ='store_nbr')\n","train_merged = train_merged.sort_values([\"store_nbr\",\"family\",\"date\"])\n","train_merged = train_merged.astype({\"store_nbr\":'str', \"family\":'str', \"city\":'str',\n","                          \"state\":'str', \"type\":'str', \"cluster\":'str'})\n","\n","df_test_dropped = df_test.drop(['onpromotion'], axis=1)\n","df_test_sorted = df_test_dropped.sort_values(by=['store_nbr','family'])\n","\n","# Create TimeSeries objects (Darts) and arrange in a Dictionary clustered by Product Family\n","\n","family_TS_dict = {}\n","\n","for family in family_list:\n","  df_family = train_merged.loc[train_merged['family'] == family]\n","\n","  list_of_TS_family = TimeSeries.from_group_dataframe(\n","                                df_family,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n","                                value_cols=\"sales\", # target variable\n","                                fill_missing_dates=True,\n","                                freq='D')\n","  for ts in list_of_TS_family:\n","            ts = ts.astype(np.float32)\n","\n","  list_of_TS_family = sorted(list_of_TS_family, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n","  family_TS_dict[family] = list_of_TS_family\n","\n","# Transform the Sales Data\n","\n","family_pipeline_dict = {}\n","family_TS_transformed_dict = {}\n","\n","for key in family_TS_dict:\n","  train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","  static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n","  log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","  train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","  train_pipeline = Pipeline([train_filler,\n","                             static_cov_transformer,\n","                             log_transformer,\n","                             train_scaler])\n","     \n","  training_transformed = train_pipeline.fit_transform(family_TS_dict[key])\n","  family_pipeline_dict[key] = train_pipeline\n","  family_TS_transformed_dict[key] = training_transformed\n","\n","# Create TimeSeries objects (Darts) 1782\n","\n","list_of_TS = TimeSeries.from_group_dataframe(\n","                                train_merged,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n","                                value_cols=\"sales\", # target variable\n","                                fill_missing_dates=True,\n","                                freq='D')\n","for ts in list_of_TS:\n","            ts = ts.astype(np.float32)\n","\n","list_of_TS = sorted(list_of_TS, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n","\n","# Transform the Sales Data\n","\n","train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n","log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_pipeline = Pipeline([train_filler,\n","                             static_cov_transformer,\n","                             log_transformer,\n","                             train_scaler])\n","     \n","training_transformed = train_pipeline.fit_transform(list_of_TS)\n","\n","#train_merged.head()\n","\n","# Create 7-day and 28-day moving average of sales\n","\n","sales_moving_average_7 = MovingAverage(window=7)\n","sales_moving_average_28 = MovingAverage(window=28)\n","\n","sales_moving_averages_dict = {}\n","\n","for key in family_TS_transformed_dict:\n","  sales_mas_family = []\n","  \n","  for ts in family_TS_transformed_dict[key]:\n","    ma_7 = sales_moving_average_7.filter(ts)\n","    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n","    ma_7 = ma_7.astype(np.float32)\n","    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"sales_ma_7\")\n","    ma_28 = sales_moving_average_28.filter(ts)\n","    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n","    ma_28 = ma_28.astype(np.float32)\n","    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"sales_ma_28\")\n","    mas = ma_7.stack(ma_28)\n","    sales_mas_family.append(mas)\n","  \n","  sales_moving_averages_dict[key] = sales_mas_family  \n","    \n","# General Covariates (Time-Based and Oil)\n","\n","full_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n","\n","# Time-Based Covariates\n","\n","year = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\n","month = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\n","day = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\n","dayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\n","weekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\n","weekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\n","timesteps = TimeSeries.from_times_and_values(times=full_time_period,\n","                                             values=np.arange(len(full_time_period)),\n","                                             columns=[\"linear_increase\"])\n","\n","time_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\n","time_cov = time_cov.astype(np.float32)\n","\n","# Transform\n","time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","time_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\n","time_cov_scaler.fit(time_cov_train)\n","time_cov_transformed = time_cov_scaler.transform(time_cov)\n","\n","#time_cov_transformed[-50:].plot()\n","\n","# Oil Price\n","\n","oil = TimeSeries.from_dataframe(df_oil, \n","                                time_col = 'date', \n","                                value_cols = ['dcoilwtico'],\n","                                freq = 'D')\n","\n","oil = oil.astype(np.float32)\n","\n","# Transform\n","oil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","oil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","oil_pipeline = Pipeline([oil_filler, oil_scaler])\n","oil_transformed = oil_pipeline.fit_transform(oil)\n","\n","# Moving Averages for Oil Price\n","oil_moving_average_7 = MovingAverage(window=7)\n","oil_moving_average_28 = MovingAverage(window=28)\n","\n","oil_moving_averages = []\n","\n","ma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\n","ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\n","ma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\n","ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\n","oil_moving_averages = ma_7.stack(ma_28)\n","\n","# Stack General Covariates Together\n","\n","general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)\n","\n","# Store-Specific Covariates (Transactions and Holidays)\n","\n","# Transactions\n","df_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\n","\n","TS_transactions_list = TimeSeries.from_group_dataframe(\n","                                df_transactions,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                value_cols=\"transactions\",\n","                                fill_missing_dates=True,\n","                                freq='D')\n","\n","transactions_list = []\n","\n","for ts in TS_transactions_list:\n","            series = TimeSeries.from_series(ts.pd_series())   # necessary workaround to remove static covariates (so I can stack covariates later on)\n","            series = series.astype(np.float32)\n","            transactions_list.append(series)\n","\n","transactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n","\n","from datetime import datetime, timedelta\n","\n","transactions_list_full = []\n","\n","for ts in transactions_list:\n","  if ts.start_time() > pd.Timestamp('20130101'):\n","    end_time = (ts.start_time() - timedelta(days=1))\n","    delta = end_time - pd.Timestamp('20130101')\n","    zero_series = TimeSeries.from_times_and_values(\n","                              times=pd.date_range(start=pd.Timestamp('20130101'), \n","                              end=end_time, freq=\"D\"),\n","                              values=np.zeros(delta.days+1))\n","    ts = zero_series.append(ts)\n","    transactions_list_full.append(ts)\n","\n","transactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","transactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","\n","transactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\n","transactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)\n","\n","# Moving Averages for Transactions\n","trans_moving_average_7 = MovingAverage(window=7)\n","trans_moving_average_28 = MovingAverage(window=28)\n","\n","transactions_covs = []\n","\n","for ts in transactions_transformed:\n","  ma_7 = trans_moving_average_7.filter(ts).astype(np.float32)\n","  ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"transactions_ma_7\")\n","  ma_28 = trans_moving_average_28.filter(ts).astype(np.float32)\n","  ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"transactions_ma_28\")\n","  trans_and_mas = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\").stack(ma_7).stack(ma_28)\n","  transactions_covs.append(trans_and_mas)\n","\n","\n","\n","# Re-Defining Categories of Holidays in a Meaningful Way\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['transferred'] == True,'Transferred', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Transfer','Holiday', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Additional','Holiday', \n","                                      df_holidays_events['type'])\n","\n","df_holidays_events['type'] = np.where(df_holidays_events['type'] == 'Bridge','Holiday', \n","                                      df_holidays_events['type'])\n","\n","\n","# Assign Holidays to all TimeSeries and Save in Dictionary\n","\n","def holiday_list(df_stores):\n","\n","    listofseries = []\n","    \n","    for i in range(0,len(df_stores)):\n","            \n","            df_holiday_dummies = pd.DataFrame(columns=['date'])\n","            df_holiday_dummies[\"date\"] = df_holidays_events[\"date\"]\n","            \n","            df_holiday_dummies[\"national_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & (df_holidays_events[\"locale\"] == \"National\")), 1, 0)\n","\n","            df_holiday_dummies[\"earthquake_relief\"] = np.where(df_holidays_events['description'].str.contains('Terremoto Manabi'), 1, 0)\n","\n","            df_holiday_dummies[\"christmas\"] = np.where(df_holidays_events['description'].str.contains('Navidad'), 1, 0)\n","\n","            df_holiday_dummies[\"football_event\"] = np.where(df_holidays_events['description'].str.contains('futbol'), 1, 0)\n","\n","            df_holiday_dummies[\"national_event\"] = np.where(((df_holidays_events[\"type\"] == \"Event\") & (df_holidays_events[\"locale\"] == \"National\") & (~df_holidays_events['description'].str.contains('Terremoto Manabi')) & (~df_holidays_events['description'].str.contains('futbol'))), 1, 0)\n","\n","            df_holiday_dummies[\"work_day\"] = np.where((df_holidays_events[\"type\"] == \"Work Day\"), 1, 0)\n","\n","            df_holiday_dummies[\"local_holiday\"] = np.where(((df_holidays_events[\"type\"] == \"Holiday\") & ((df_holidays_events[\"locale_name\"] == df_stores['state'][i]) | (df_holidays_events[\"locale_name\"] == df_stores['city'][i]))), 1, 0)\n","                     \n","            listofseries.append(df_holiday_dummies)\n","\n","    return listofseries\n","\n","def remove_0_and_duplicates(holiday_list):\n","\n","    listofseries = []\n","    \n","    for i in range(0,len(holiday_list)):\n","            \n","            df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n","\n","            df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]\n","            \n","            df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max', \n","                                   'christmas':'max', 'football_event':'max', \n","                                   'national_event':'max', 'work_day':'max', \n","                                   'local_holiday':'max'}).reset_index()\n","\n","            listofseries.append(df_holiday_per_store)\n","\n","    return listofseries\n","\n","def holiday_TS_list_54(holiday_list):\n","\n","    listofseries = []\n","    \n","    for i in range(0,54):\n","            \n","            holidays_TS = TimeSeries.from_dataframe(list_of_holidays_per_store[i], \n","                                        time_col = 'date',\n","                                        fill_missing_dates=True,\n","                                        fillna_value=0,\n","                                        freq='D')\n","            \n","            holidays_TS = holidays_TS.slice(pd.Timestamp('20130101'),pd.Timestamp('20170831'))\n","            holidays_TS = holidays_TS.astype(np.float32)\n","            listofseries.append(holidays_TS)\n","\n","    return listofseries\n","\n","\n","list_of_holidays_per_store = holiday_list(df_stores)\n","list_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)   \n","list_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\n","\n","holidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n","holidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n","\n","holidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\n","holidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)\n","\n","# Stack Together Store-Specific Covariates with General Covariates\n","\n","store_covariates_future = []\n","\n","for store in range(0,len(store_list)):\n","  stacked_covariates = holidays_transformed[store].stack(general_covariates)  \n","  store_covariates_future.append(stacked_covariates)\n","\n","store_covariates_past = []\n","holidays_transformed_sliced = holidays_transformed # for slicing past covariates\n","\n","for store in range(0,len(store_list)):\n","  holidays_transformed_sliced[store] = holidays_transformed[store].slice_intersect(transactions_covs[store])\n","  general_covariates_sliced = general_covariates.slice_intersect(transactions_covs[store])\n","  stacked_covariates = transactions_covs[store].stack(holidays_transformed_sliced[store]).stack(general_covariates_sliced)  \n","  store_covariates_past.append(stacked_covariates)\n","    \n","# Store/Family-Varying Covariates (Promotion)\n","\n","df_promotion = pd.concat([df_train, df_test], axis=0)\n","df_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\n","df_promotion.tail()\n","\n","family_promotion_dict = {}\n","\n","for family in family_list:\n","  df_family = df_promotion.loc[df_promotion['family'] == family]\n","\n","  list_of_TS_promo = TimeSeries.from_group_dataframe(\n","                                df_family,\n","                                time_col=\"date\",\n","                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n","                                value_cols=\"onpromotion\", # covariate of interest\n","                                fill_missing_dates=True,\n","                                freq='D')\n","  \n","  for ts in list_of_TS_promo:\n","            ts = ts.astype(np.float32)\n","\n","  family_promotion_dict[family] = list_of_TS_promo\n","\n","promotion_transformed_dict = {}\n","\n","for key in tqdm(family_promotion_dict):\n","  promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","  promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","  promo_pipeline = Pipeline([promo_filler,\n","                             promo_scaler])\n","  \n","  promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n","\n","  # Moving Averages for Promotion Family Dictionaries\n","  promo_moving_average_7 = MovingAverage(window=7)\n","  promo_moving_average_28 = MovingAverage(window=28)\n","\n","  promotion_covs = []\n","\n","  for ts in promotion_transformed:\n","    ma_7 = promo_moving_average_7.filter(ts)\n","    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n","    ma_7 = ma_7.astype(np.float32)\n","    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n","    ma_28 = promo_moving_average_28.filter(ts)\n","    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n","    ma_28 = ma_28.astype(np.float32)\n","    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n","    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n","    promotion_covs.append(promo_and_mas)\n","\n","  promotion_transformed_dict[key] = promotion_covs\n","\n","# 2.5. Assemble All Covariates in Dictionaries\n","\n","past_covariates_dict = {}\n","\n","for key in tqdm(promotion_transformed_dict):\n","\n","  promotion_family = promotion_transformed_dict[key]\n","  sales_mas = sales_moving_averages_dict[key]\n","  covariates_past = [promotion_family[i].slice_intersect(store_covariates_past[i]).stack(store_covariates_past[i].stack(sales_mas[i])) for i in range(0,len(promotion_family))]\n","\n","  past_covariates_dict[key] = covariates_past\n","\n","future_covariates_dict = {}\n","\n","for key in tqdm(promotion_transformed_dict):\n","\n","  promotion_family = promotion_transformed_dict[key]\n","  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n","\n","  future_covariates_dict[key] = covariates_future\n","\n","only_past_covariates_dict = {}\n","\n","for key in tqdm(sales_moving_averages_dict):\n","  sales_moving_averages = sales_moving_averages_dict[key]\n","  only_past_covariates = [sales_moving_averages[i].stack(transactions_covs[i]) for i in range(0,len(sales_moving_averages))]\n","\n","  only_past_covariates_dict[key] = only_past_covariates\n","\n","# Delete Original Dataframes to Save Memory\n","\n","del(df_train)\n","del(df_test)\n","del(df_stores)\n","del(df_holidays_events)\n","del(df_oil)\n","del(df_transactions)\n","gc.collect()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"2.2.\"></a> <br>\n","# 2.2. EDA"]},{"cell_type":"markdown","metadata":{},"source":["For a first impression, let's look at a few of the 1782 (store x product family) timeseries:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-10-25T08:30:33.746539Z","iopub.status.busy":"2022-10-25T08:30:33.746012Z","iopub.status.idle":"2022-10-25T08:30:34.603037Z","shell.execute_reply":"2022-10-25T08:30:34.602081Z","shell.execute_reply.started":"2022-10-25T08:30:33.746507Z"},"trusted":true},"outputs":[],"source":["# Some EDA\n","\n","bread_series = family_TS_dict['BREAD/BAKERY'][0]\n","celebration_series = family_TS_dict['CELEBRATION'][11]\n","\n","\n","# Let's print two of the 1782 TimeSeries\n","\n","plt.subplots(2, 2, figsize=(15, 6))\n","plt.subplot(1, 2, 1) # row 1, col 2 index 1\n","bread_series.plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","celebration_series.plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Two Out Of 1782 TimeSeries\")\n","           \n","plt.subplot(1, 2, 2) # index 2\n","bread_series[-365:].plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","celebration_series[-365:].plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Only The Last 365 Days\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's also plot the ACF for both series and investigate the seasonal patterns:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:30:37.391919Z","iopub.status.busy":"2022-10-25T08:30:37.391568Z","iopub.status.idle":"2022-10-25T08:30:37.9299Z","shell.execute_reply":"2022-10-25T08:30:37.928947Z","shell.execute_reply.started":"2022-10-25T08:30:37.391889Z"},"trusted":true},"outputs":[],"source":["# Inspect Seasonality\n","\n","plot_acf(fill_missing_values(bread_series), m=7, alpha=0.05)\n","plt.title(\"{}, store {} in {}\".format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","plot_acf(fill_missing_values(celebration_series), alpha=0.05)\n","plt.title(\"{}, store {} in {}\".format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]));"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, the BREAD/BAKERY series displays strong weekly seasonality, as we would expect. The CELEBRATION series however has a much less clear seasonal pattern. \n","\n","I encoded the static covariates and applied 0-1 Scaling + Log-Transformation to all series. Static covariates don't vary over time - examples in our dataset are the store number or region. Scaling is important for many of the deep learning models, and the logarithmic transformation of the training data will help against undershooting the actual sales with our forecasts."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:30:40.858518Z","iopub.status.busy":"2022-10-25T08:30:40.858006Z","iopub.status.idle":"2022-10-25T08:30:48.596174Z","shell.execute_reply":"2022-10-25T08:30:48.594915Z","shell.execute_reply.started":"2022-10-25T08:30:40.858486Z"},"trusted":true},"outputs":[],"source":["# Show the Differenced Series\n","\n","# First Transform the Example Series\n","train_filler_bread = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer_bread = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") \n","log_transformer_bread = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler_bread = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_filler_celebration = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n","static_cov_transformer_celebration = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") \n","log_transformer_celebration = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n","train_scaler_celebration = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n","\n","train_pipeline_bread = Pipeline([train_filler_bread,\n","                             static_cov_transformer_bread,\n","                             log_transformer_bread,\n","                             train_scaler_bread])\n","\n","train_pipeline_celebration = Pipeline([train_filler_celebration,\n","                             static_cov_transformer_celebration,\n","                             log_transformer_celebration,\n","                             train_scaler_celebration])\n","     \n","bread_series_transformed = train_pipeline_bread.fit_transform(bread_series)\n","celebration_series_transformed = train_pipeline_celebration.fit_transform(celebration_series)\n","\n","# Plots\n","\n","plt.subplots(2, 2, figsize=(15, 6))\n","plt.subplot(1, 2, 1) # row 1, col 2 index 1\n","bread_series_transformed.plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"TimeSeries After Scaling and Log-Transform\")\n","           \n","plt.subplot(1, 2, 2) # index 2\n","bread_series_transformed[-365:].plot(label='Sales for {}'.format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Only The Last 365 Days\")\n","plt.show()\n","\n","plt.subplots(2, 2, figsize=(15, 6))\n","plt.subplot(1, 2, 1) # row 1, col 2 index 1\n","celebration_series_transformed.plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"TimeSeries After Scaling and Log-Transform\")\n","           \n","plt.subplot(1, 2, 2) # index 2\n","celebration_series_transformed[-365:].plot(label='Sales for {}'.format(celebration_series.static_covariates_values()[0,1], \n","                                                celebration_series.static_covariates_values()[0,0],\n","                                                celebration_series.static_covariates_values()[0,2]))\n","\n","plt.title(\"Only The Last 365 Days\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Covariates\n","\n","Let's look at the covariates for the last 180 days of the BREAD/BAKERY series in store 1.\n","\n","### Sales Moving Average Terms\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:14:34.165177Z","iopub.status.busy":"2022-10-25T08:14:34.164793Z","iopub.status.idle":"2022-10-25T08:14:34.550614Z","shell.execute_reply":"2022-10-25T08:14:34.549307Z","shell.execute_reply.started":"2022-10-25T08:14:34.165146Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","family_TS_transformed_dict['BREAD/BAKERY'][0][-180:].plot()\n","sales_moving_averages_dict['BREAD/BAKERY'][0][-180:].plot()\n","plt.title(\"Sales 7- and 28-day Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Promotion Data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:15:04.351471Z","iopub.status.busy":"2022-10-25T08:15:04.350799Z","iopub.status.idle":"2022-10-25T08:15:04.754443Z","shell.execute_reply":"2022-10-25T08:15:04.753522Z","shell.execute_reply.started":"2022-10-25T08:15:04.351436Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","promotion_transformed_dict['BREAD/BAKERY'][0][-180:].plot()\n","plt.title(\"Promotion Data and Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Transactions"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:15:10.690661Z","iopub.status.busy":"2022-10-25T08:15:10.690112Z","iopub.status.idle":"2022-10-25T08:15:11.168121Z","shell.execute_reply":"2022-10-25T08:15:11.167007Z","shell.execute_reply.started":"2022-10-25T08:15:10.69062Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","transactions_covs[0][-180:].plot()\n","plt.title(\"Transactions Data and Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Oil Price\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:15:24.356612Z","iopub.status.busy":"2022-10-25T08:15:24.35625Z","iopub.status.idle":"2022-10-25T08:15:24.900334Z","shell.execute_reply":"2022-10-25T08:15:24.89402Z","shell.execute_reply.started":"2022-10-25T08:15:24.356582Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","oil_transformed[-180:].plot()\n","oil_moving_averages[-180:].plot()\n","plt.title(\"Oil Price and Moving Averages\");"]},{"cell_type":"markdown","metadata":{},"source":["### Time Dummies and Covariates\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-11-17T01:22:45.246694Z","iopub.status.busy":"2022-11-17T01:22:45.246244Z","iopub.status.idle":"2022-11-17T01:22:45.833908Z","shell.execute_reply":"2022-11-17T01:22:45.832889Z","shell.execute_reply.started":"2022-11-17T01:22:45.246655Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","time_cov_transformed[-180:].plot()\n","plt.title(\"Time-Related Covariates\");"]},{"cell_type":"markdown","metadata":{},"source":["### Holidays and Events\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["I ordered the available data on holidays in the following seven categories. I think it might work better to generalize the categories further than what I did here."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T01:22:16.74045Z","iopub.status.busy":"2022-11-17T01:22:16.740091Z","iopub.status.idle":"2022-11-17T01:22:16.993646Z","shell.execute_reply":"2022-11-17T01:22:16.989263Z","shell.execute_reply.started":"2022-11-17T01:22:16.740421Z"},"trusted":true},"outputs":[],"source":["#df_holidays_events['type'].value_counts().plot.bar(rot=0)\n","plt.figure(figsize=(10, 6))\n","list_of_holidays_per_store[0].loc[:, list_of_holidays_per_store[0].columns != \"date\"].sum().plot.bar(rot=0)\n","plt.title(\"Holidays and Events\");"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":2887556,"sourceId":29781,"sourceType":"competition"}],"dockerImageVersionId":30236,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"torch-gpu","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}
