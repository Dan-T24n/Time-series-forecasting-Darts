{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **TL;DR:** \n","   \n","    \n","**<ins>Baseline</ins>**: \n","    \n","* **Exponential Smoothing** applied to every single series generates a good baseline forecast - **RMSLE: 0.40578**\n"," <br />\n","    \n","    \n","ðŸ“Œ **<ins>Best Model</ins>**: \n","    \n","* **33 Global LightGBM Models**, one for every product family\n","* **Each trained on 54 timeseries** (for 54 stores)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"markdown","metadata":{},"source":["\n","# Simple Baseline Models"]},{"cell_type":"markdown","metadata":{},"source":["Before thinking about neural networks, I started by setting a baseline with traditional and more simple methods. The baseline forecast performance then constitutes a lower bound for the Machine Learning models' expected performance. I used three simple-to-implement models:\n","\n","* Naive Seasonal Model (repeating the last 7 days)\n","* Exponential Smoothing\n","* Facebook Prophet\n","\n","**Exponential Smoothing** gave the best results. For computational reasons, I commented out the other two models' training/evaluation later on."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"3.1.\"></a> <br>\n","# 3.1. Some Quick Backtests"]},{"cell_type":"markdown","metadata":{},"source":["Let's quickly look at **backtests (historical forecasts)** for two individual series from our large dataset. We start with the sales for **Bread and Bakery in store 1** - one of the more consistent and seasonal series in our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:31:02.255911Z","iopub.status.busy":"2022-10-25T08:31:02.255498Z","iopub.status.idle":"2022-10-25T08:31:26.818711Z","shell.execute_reply":"2022-10-25T08:31:26.817559Z","shell.execute_reply.started":"2022-10-25T08:31:02.255876Z"},"trusted":true},"outputs":[],"source":["\n","# # Define Models\n","\n","# from darts.models import NaiveSeasonal, ExponentialSmoothing, Prophet\n","# from darts.timeseries import concatenate\n","# import logging\n","# cmdstanpy_logger = logging.getLogger(\"cmdstanpy\")\n","# cmdstanpy_logger.disabled = True\n","\n","# Naive_Seasonal_Model = NaiveSeasonal(K=7)\n","\n","# Exponential_Smoothing_Model = ExponentialSmoothing()\n","\n","# Prophet_Model = Prophet()\n","\n","# def eval_backtest(backtest_series, actual_series, horizon, transformer, model):\n","#     actualdata = transformer.inverse_transform(actual_series, partial=True)\n","#     forecasts = transformer.inverse_transform(backtest_series, partial=True)\n","#     plt.figure(figsize=(10, 6))\n","#     actualdata[-365:].plot(label=\"Actual Data\")\n","#     forecasts.plot(label=model)\n","#     plt.legend()\n","#     plt.suptitle(\"{} in store {} ({})\".format(static_cov_transformer_bread.inverse_transform(actual_series).static_covariates_values()[0,1], \n","#                                                 static_cov_transformer_bread.inverse_transform(actual_series).static_covariates_values()[0,0],\n","#                                                 static_cov_transformer_bread.inverse_transform(actual_series).static_covariates_values()[0,2]))\n","#     plt.title(\"Backtest with {}-months horizon, RMSLE = {:.2f}\".format(horizon,\n","#             rmsle(actual_series=actualdata, pred_series=forecasts)))    \n","    \n","# backtest_series_SN = Naive_Seasonal_Model.historical_forecasts(\n","#     bread_series_transformed,\n","#     start=pd.Timestamp('20161101'),\n","#     forecast_horizon=16,\n","#     stride=16,\n","#     last_points_only=False,\n","#     retrain=True,\n","#     verbose=False,\n","# )\n","\n","# backtest_series_ES = Exponential_Smoothing_Model.historical_forecasts(\n","#     bread_series_transformed,\n","#     start=pd.Timestamp('20161101'),\n","#     forecast_horizon=16,\n","#     stride=16,\n","#     last_points_only=False,\n","#     retrain=True,\n","#     verbose=False,\n","# )\n","\n","# backtest_series_Prophet = Prophet_Model.historical_forecasts(\n","#     bread_series_transformed,\n","#     start=pd.Timestamp('20161101'),\n","#     forecast_horizon=16,\n","#     stride=16,\n","#     last_points_only=False,\n","#     retrain=True,\n","#     verbose=False,\n","# )\n","\n","\n","# eval_backtest(\n","#     backtest_series=concatenate(backtest_series_SN),\n","#     actual_series=bread_series_transformed,\n","#     horizon=16,\n","#     transformer=train_pipeline_bread,\n","#     model=\"Seasonal Naive (K=7) Forecasts\"\n","# )\n","\n","# eval_backtest(\n","#     backtest_series=concatenate(backtest_series_ES),\n","#     actual_series=bread_series_transformed,\n","#     horizon=16,\n","#     transformer=train_pipeline_bread,\n","#     model=\"Exponential Smoothing Forecasts\"\n","# )\n","\n","# eval_backtest(\n","#     backtest_series=concatenate(backtest_series_Prophet),\n","#     actual_series=bread_series_transformed,\n","#     horizon=16,\n","#     transformer=train_pipeline_bread,\n","#     model=\"Facebook Prophet Forecasts\"\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["Except for the zero sales around christmas/new year 2017, the series follows a **very consistent pattern** - people eat bread every week. All three models are able to predict this pattern pretty well. The Seasonal Naive model however fails heavily in the aftermath of this one observed downward spike.\n","\n","Let's now look at a **more complicated series** - as an example I picked the product family **CELEBRATION in store 19**. Though this series also has seasonal patterns, it displays much **more spikes**. How will our baseline models fare now?"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:31:53.966042Z","iopub.status.busy":"2022-10-25T08:31:53.964998Z","iopub.status.idle":"2022-10-25T08:32:16.093486Z","shell.execute_reply":"2022-10-25T08:32:16.092344Z","shell.execute_reply.started":"2022-10-25T08:31:53.965996Z"},"trusted":true},"outputs":[],"source":["\n","\n","# def eval_backtest(backtest_series, actual_series, horizon, transformer, model):\n","#     actualdata = transformer.inverse_transform(actual_series, partial=True)\n","#     forecasts = transformer.inverse_transform(backtest_series, partial=True)\n","#     plt.figure(figsize=(10, 6))\n","#     actualdata[-365:].plot(label=\"Actual Data\")\n","#     forecasts.plot(label=model)\n","#     plt.legend()\n","#     plt.suptitle(\"{} in store {} ({})\".format(static_cov_transformer_celebration.inverse_transform(actual_series).static_covariates_values()[0,1], \n","#                                                 static_cov_transformer_celebration.inverse_transform(actual_series).static_covariates_values()[0,0],\n","#                                                 static_cov_transformer_celebration.inverse_transform(actual_series).static_covariates_values()[0,2]))\n","#     plt.title(\"Backtest with {}-months horizon, RMSLE = {:.2f}\".format(horizon,\n","#             rmsle(actual_series=actualdata, pred_series=forecasts)))    \n","\n","# backtest_series_SN_2 = Naive_Seasonal_Model.historical_forecasts(\n","#     celebration_series_transformed,\n","#     start=pd.Timestamp('20161101'),\n","#     forecast_horizon=16,\n","#     stride=16,\n","#     last_points_only=False,\n","#     retrain=True,\n","#     verbose=False,\n","# )\n","\n","# backtest_series_ES_2 = Exponential_Smoothing_Model.historical_forecasts(\n","#     celebration_series_transformed,\n","#     start=pd.Timestamp('20161101'),\n","#     forecast_horizon=16,\n","#     stride=16,\n","#     last_points_only=False,\n","#     retrain=True,\n","#     verbose=False,\n","# )\n","\n","# backtest_series_Prophet_2 = Prophet_Model.historical_forecasts(\n","#     celebration_series_transformed,\n","#     start=pd.Timestamp('20161101'),\n","#     forecast_horizon=16,\n","#     stride=16,\n","#     last_points_only=False,\n","#     retrain=True,\n","#     verbose=False,\n","# )\n","\n","\n","# eval_backtest(\n","#     backtest_series=concatenate(backtest_series_SN_2),\n","#     actual_series=celebration_series_transformed,\n","#     horizon=16,\n","#     transformer=train_pipeline_celebration,\n","#     model=\"Seasonal Naive (K=7) Forecasts\"\n","# )\n","\n","# eval_backtest(\n","#     backtest_series=concatenate(backtest_series_ES_2),\n","#     actual_series=celebration_series_transformed,\n","#     horizon=16,\n","#     transformer=train_pipeline_celebration,\n","#     model=\"Exponential Smoothing Forecasts\"\n","# )\n","\n","# eval_backtest(\n","#     backtest_series=concatenate(backtest_series_Prophet_2),\n","#     actual_series=celebration_series_transformed,\n","#     horizon=16,\n","#     transformer=train_pipeline_celebration,\n","#     model=\"Facebook Prophet Forecasts\"\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["Those forecasts are not ideal - predicting sales is not that easy after all. The spikes in \"CELEBRATION\" products are likely caused by special events. While the Seasonal Naive model generates many falsely predicted spikes (and misses most of the actual ones), the Exponential Smoothing and Prophet models capture underlying seasonal patterns and trends, but miss all spikes. Following those forecasts, our supermarkets would not carry enough celebration merchandise when it is demanded the most, missing a lot of profit.\n","\n","From this little experiment, we can already see that good models must both capture the general seasonalities and trends in product sales, as well as understand predictable spikes and other special patterns. As we have data on holidays for example, that should hopefully be possible to some extent."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"3.2.\"></a> <br>\n","# 3.2. Training/Test Split Performance Comparison"]},{"cell_type":"markdown","metadata":{},"source":["From now on, I will use a **simple training/test split** to evaluate models on all series - the forecast horizon will be **16 days** in order to mimic the leaderboard prediction task. While rolling window validation (as in the backtests before) is a more reliable approach, it would require too much computational resources during the experimental phase of the forecasting project. Once we start training neural network models, this will become obvious."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:32:21.369096Z","iopub.status.busy":"2022-10-25T08:32:21.36869Z","iopub.status.idle":"2022-10-25T08:32:21.850017Z","shell.execute_reply":"2022-10-25T08:32:21.849023Z","shell.execute_reply.started":"2022-10-25T08:32:21.36906Z"},"trusted":true},"outputs":[],"source":["# Show example training and validation set for target series\n","\n","training_series_bread = bread_series_transformed[:-16]\n","val_series_bread = bread_series_transformed[-16:]\n","\n","plt.figure(figsize=(10, 6))\n","training_series_bread[-100:].plot(label='Training')\n","val_series_bread.plot(label='Validation')\n","plt.legend()\n","plt.title(\"{} in store {} ({})\".format(bread_series.static_covariates_values()[0,1], \n","                                                bread_series.static_covariates_values()[0,0],\n","                                                bread_series.static_covariates_values()[0,2]));"]},{"cell_type":"markdown","metadata":{},"source":["For our baseline methods, we now train **1782 models, one for each (store x family) TimeSeries**. The forecasts created by these models will then be transformed back to the original scale. Additionally, we predict pure zero-forecasts for all series, which had no sales in the last two weeks (somewhat arbitrary choice)."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T08:32:40.811008Z","iopub.status.busy":"2022-10-25T08:32:40.810619Z","iopub.status.idle":"2022-10-25T08:43:00.414694Z","shell.execute_reply":"2022-10-25T08:43:00.412906Z","shell.execute_reply.started":"2022-10-25T08:32:40.810969Z"},"trusted":true},"outputs":[],"source":["# Functions for Exponential Smoothing Models and Forecasts\n","\n","def ESModelBuilder(training_list):\n","\n","    listofESmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        ES_model = ExponentialSmoothing()\n","        ES_model.fit(training_list[i])\n","        listofESmodels.append(ES_model)\n","\n","    return listofESmodels \n","\n","def ESForecaster(model_list):\n","\n","    listofESpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_ES = model_list[i].predict(n=16)\n","        listofESpreds.append(pred_ES)        \n","\n","    return listofESpreds \n","\n","# Train and Forecast with Exponential Smoothing Models\n","\n","ES_Models_Family_Dict = {}\n","ES_Forecasts_Family_Dict = {}\n","\n","# get the start time\n","st = time.time()\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  ES_Models_Family_Dict[family] = ESModelBuilder(training_data)\n","  forecasts_ES = ESForecaster(ES_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  ES_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_ES, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(ES_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-14:] == 0).all():\n","          ES_Forecasts_Family_Dict[family][i] = ES_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_exp = et - st\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-19T22:47:03.227116Z","iopub.status.busy":"2022-09-19T22:47:03.226037Z","iopub.status.idle":"2022-09-19T22:59:18.914242Z","shell.execute_reply":"2022-09-19T22:59:18.912998Z","shell.execute_reply.started":"2022-09-19T22:47:03.227037Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Functions for Exponential Smoothing Models and Forecasts\n","\n","def NSModelBuilder(training_list):\n","\n","    listofNSmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        NS_model = NaiveSeasonal(K=7)\n","        NS_model.fit(training_list[i])\n","        listofNSmodels.append(NS_model)\n","\n","    return listofNSmodels \n","\n","def NSForecaster(model_list):\n","\n","    listofNSpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_NS = model_list[i].predict(n=16)\n","        listofNSpreds.append(pred_NS)        \n","\n","    return listofNSpreds \n","\n","def ESModelBuilder(training_list):\n","\n","    listofESmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        ES_model = ExponentialSmoothing()\n","        ES_model.fit(training_list[i])\n","        listofESmodels.append(ES_model)\n","\n","    return listofESmodels \n","\n","def ESForecaster(model_list):\n","\n","    listofESpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_ES = model_list[i].predict(n=16)\n","        listofESpreds.append(pred_ES)        \n","\n","    return listofESpreds \n","\n","def ProphetModelBuilder(training_list):\n","\n","    listofPmodels = []\n","\n","    for i in range(0,len(training_list)):\n","        P_model = Prophet()\n","        P_model.fit(training_list[i])\n","        listofPmodels.append(P_model)\n","\n","    return listofPmodels \n","\n","def ProphetForecaster(model_list):\n","\n","    listofPpreds = []\n","\n","    for i in range(0,len(model_list)):\n","        pred_P = model_list[i].predict(n=16)\n","        listofPpreds.append(pred_P)        \n","\n","    return listofPpreds \n","\n","# Train and Forecast with Exponential Smoothing Models\n","\n","NS_Models_Family_Dict = {}\n","NS_Forecasts_Family_Dict = {}\n","ES_Models_Family_Dict = {}\n","ES_Forecasts_Family_Dict = {}\n","Prophet_Models_Family_Dict = {}\n","Prophet_Forecasts_Family_Dict = {}\n","\n","import time\n","from multiprocessing import Pool\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  NS_Models_Family_Dict[family] = NSModelBuilder(training_data)\n","  forecasts_NS = NSForecaster(NS_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  NS_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_NS, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(NS_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          NS_Forecasts_Family_Dict[family][i] = NS_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","    \n","    \n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  ES_Models_Family_Dict[family] = ESModelBuilder(training_data)\n","  forecasts_ES = ESForecaster(ES_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  ES_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_ES, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(ES_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          ES_Forecasts_Family_Dict[family][i] = ES_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","    \n"," # commented out due to long computation\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","\n","  Prophet_Models_Family_Dict[family] = ProphetModelBuilder(training_data)\n","  forecasts_Prophet = ProphetForecaster(Prophet_Models_Family_Dict[family])\n","    \n","  # Transform Back\n","  Prophet_Forecasts_Family_Dict[family] = family_pipeline_dict[family].inverse_transform(forecasts_Prophet, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(Prophet_Forecasts_Family_Dict[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          Prophet_Forecasts_Family_Dict[family][i] = Prophet_Forecasts_Family_Dict[family][i].map(lambda x: x * 0)\n","          \"\"\" "]},{"cell_type":"markdown","metadata":{},"source":["Let's check the RMSLE scores on the 16-days validation set we created:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-10-25T08:47:20.200184Z","iopub.status.busy":"2022-10-25T08:47:20.199792Z","iopub.status.idle":"2022-10-25T08:47:49.282278Z","shell.execute_reply":"2022-10-25T08:47:49.281087Z","shell.execute_reply.started":"2022-10-25T08:47:20.200151Z"},"trusted":true},"outputs":[],"source":["# Re-Format Forecasts from Dictionaries to One List\n","\n","forecast_list_ES = []\n","\n","for family in family_list:\n","  forecast_list_ES.append(ES_Forecasts_Family_Dict[family])\n","\n","sales_data = []\n","\n","for family in family_list:\n","  sales_data.append(family_TS_dict[family])\n","\n","# Function to Flatten Nested Lists\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","    \n","actual_list = flatten(sales_data)\n","pred_list_ES = flatten(forecast_list_ES)\n","\n","# Mean RMSLE\n","ES_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_ES,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","print(\"\\n\")\n","print(\"The mean RMSLE for the Local Exponential Smoothing Models over 1782 series is {:.5f}.\".format(ES_rmsle))\n","print('Training & Inference duration:', elapsed_time_exp, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-19T23:07:22.692446Z","iopub.status.busy":"2022-09-19T23:07:22.692005Z","iopub.status.idle":"2022-09-19T23:07:22.713172Z","shell.execute_reply":"2022-09-19T23:07:22.712236Z","shell.execute_reply.started":"2022-09-19T23:07:22.69241Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","\n","# Re-Format Forecasts from Dictionaries to One List\n","\n","forecast_list_NS = []\n","\n","for family in tqdm(family_list):\n","  forecast_list_NS.append(NS_Forecasts_Family_Dict[family])\n","\n","forecast_list_ES = []\n","\n","for family in tqdm(family_list):\n","  forecast_list_ES.append(ES_Forecasts_Family_Dict[family])\n","\n","#forecast_list_Prophet = []\n","\n","#for family in tqdm(family_list):\n","#  forecast_list_Prophet.append(Prophet_Forecasts_Family_Dict[family])\n","\n","sales_data = []\n","\n","for family in tqdm(family_list):\n","  sales_data.append(family_TS_dict[family])\n","\n","# Function to Flatten Nested Lists\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","    \n","actual_list = flatten(sales_data)\n","pred_list_NS = flatten(forecast_list_NS)\n","pred_list_ES = flatten(forecast_list_ES)\n","#pred_list_Prophet = flatten(forecast_list_Prophet)\n","\"\"\"\n","\n","\"\"\"\n","# Mean RMSLE\n","\n","NS_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_NS,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","ES_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_ES,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","#Prophet_rmsle = rmsle(actual_series = actual_list,\n","#                 pred_series = pred_list_Prophet,\n","#                 n_jobs = -1,\n","#                 inter_reduction=np.mean)\n","\n","print(\"The mean RMSLE for the Naive Seasonal (K=7) Model over all 1782 series is {:.5f}.\".format(NS_rmsle))\n","print(\"\\n\")\n","print(\"The mean RMSLE for Exponential Smoothing over all 1782 series is {:.5f}.\".format(ES_rmsle))\n","print(\"\\n\")\n","#print(\"The mean RMSLE for Prophet over all 1782 series is {:.5f}.\".format(Prophet_rmsle))\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["**Exponential Smoothing achieves the smallest error (RMSLE = 0.37411) for our validation data!**\n","\n","To further investigate those models' performance, let's print the **mean RMSLE for each product category**:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-10-25T08:57:26.052979Z","iopub.status.busy":"2022-10-25T08:57:26.051904Z","iopub.status.idle":"2022-10-25T08:58:02.930774Z","shell.execute_reply":"2022-10-25T08:58:02.928787Z","shell.execute_reply.started":"2022-10-25T08:57:26.052916Z"},"trusted":true},"outputs":[],"source":["# Mean RMSLE for Families\n","\n","family_forecast_rmsle_ES = {}\n","\n","for family in family_list:\n","\n","  ES_rmsle_family = rmsle(actual_series = family_TS_dict[family],\n","                 pred_series = ES_Forecasts_Family_Dict[family],\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","  \n","  family_forecast_rmsle_ES[family] = ES_rmsle_family\n","\n","\n","family_forecast_rmsle_ES = dict(sorted(family_forecast_rmsle_ES.items(), key=lambda item: item[1]))\n","\n","print(\"\\n\")\n","print(\"Mean RMSLE for the 33 different product families, from worst to best:\")\n","print(\"\\n\")\n","\n","# Iterate over key/value pairs in dict and print them\n","for key, value in family_forecast_rmsle_ES.items():\n","    print(key, ' : ', value)"]},{"cell_type":"markdown","metadata":{},"source":[" I also plot the **three worst forecasts generated** - maybe we can learn something:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-10-25T09:34:37.191009Z","iopub.status.busy":"2022-10-25T09:34:37.189752Z","iopub.status.idle":"2022-10-25T09:35:07.783836Z","shell.execute_reply":"2022-10-25T09:35:07.78292Z","shell.execute_reply.started":"2022-10-25T09:34:37.190945Z"},"trusted":true},"outputs":[],"source":["# Plot the five worst forecasts   #family_TS_dict\n","\n","errorlist = []\n","\n","for i in range(0, len(actual_list)):\n","\n","  error = rmsle(actual_series = actual_list[i], \n","                pred_series = pred_list_ES[i])\n","  \n","  errorfam = actual_list[i].static_covariates_values()[0,1]\n","  \n","  errorlist.append([errorfam,error])\n","\n","rmsle_series_ES = pd.DataFrame(errorlist,columns=['family','RMSLE'])\n","worst_3_ES = rmsle_series_ES.sort_values(by=['RMSLE'], ascending=False).head(3)\n","\n","fig,axs = plt.subplots(1,len(worst_3_ES),figsize=(20, 5))\n","labels = [\"actual data\", \"ES forecast\"]\n","for i in range(0, len(worst_3_ES)):\n","  plt_forecast = pred_list_ES[(worst_3_ES.index[i])]\n","  plt_actual = actual_list[(worst_3_ES.index[i])]\n","  plt_err = rmsle(plt_actual, plt_forecast)\n","  axis = axs[i]\n","  plt_actual[-100:].plot(ax=axis, label=\"actual data\") #, label=\"actual data\"\n","  plt_forecast.plot(ax=axis, label=\"ES forecast\") #, label=\"ES forecast\"\n","  axis.legend(loc=\"upper left\")\n","  axis.title.set_text(\"{} in store {} ({}) \\n RMSLE: {}\".format(plt_forecast.static_covariates_values()[0,1], \n","                                               plt_forecast.static_covariates_values()[0,0],\n","                                               plt_forecast.static_covariates_values()[0,2],\n","                                               plt_err))  "]},{"cell_type":"markdown","metadata":{},"source":["Apparently school has finally started;) Those forecasts are pretty bad, but the Exponential Smoothing models could not have been expected to predict a sudden peak in sales like this. My hope would be to capture such (in this case probably month/week-specific) patterns with more informed models using covariates, such as NN models."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4\"></a> <br>\n","# 4. Global Modeling"]},{"cell_type":"markdown","metadata":{},"source":["Now that we set a solid baseline, can we improve our forecasts? My current understanding of timeseries forecasting is this:\n","\n","When dealing with small datasets and few dimensions, fancy/complex models (I'm looking at you, neural networks) won't give much benefit, if any. I often found simpler statistical models outperforming them.\n","\n","Intuitively, that makes sense: Neural networks are highly complex, non-linear models that don't force any structure on the data at hand. Statistical methods like ARIMA, Exponential Smoothing or Prophet on the other hand involve a lot of fixed structure and don't offer the same amount of flexibility as neural networks or boosted tree models. But when modeling a single univariate timeseries of a couple hundred data points, we don't need all this flexbility - there is not enough signal in our data to model highly complex relationships. When it comes to capturing basic patterns like seasonality and trend, statistical methods do a great job.\n","\n","The store sales data at hand however consists of 1782 timeseries of considerable length, including a set of relevant covariates (such as holidays and product promotions). That means there should be some signal to exploit with Machine Learning models. I would expect (most of) those 1782 series to be somewhat similar/related to each other, as they all concern store sales, which should follow common patterns to some extent. So what now?\n","\n","**Global Models!** My plan is to leverage the power of NN and Boosting models to exploit/capture as much of the signal in our huge dataset as possible. Whereas local models (like our Exponential Smoothing baseline) are trained on one timeseries, global models are trained on multiple series."]},{"cell_type":"markdown","metadata":{},"source":["We will now look at three different deep learning models:\n","\n","* LSTM (Long Short-Term Memory)\n","* N-HiTS (Neural Hierarchical Interpolation for TS Forecasting)\n","* TFT (Temporal Fusion Transformer)\n","\n","The LSTM model (1995) has been around for some time, whereas TFT (2019) and N-HiTS (2022) are relatively new models. I chose these three neural network models for the different way the utilize covariates.\n","\n","LSTM is a recurrent neural network (RNN) and expects covariates that extend into the future until the forecast horizon. Within the Darts framework, those are called *future_covariates*. N-HiTS is similar to the N-BEATS model, but might offer computational benefits over it. This model can only work take in *past_covariates*, known until the point in time when a forecast is generated. Last but not least, the TFT model supports both *past_covariates* and *future_covariates* as well as *static_covariates*."]},{"cell_type":"markdown","metadata":{},"source":["Before training those models, I performed some hyperparameter tuning with the help of Python's Optuna library. As this can take quite some time for NN models, I let the tuning run with a quicker GPU on Google Colab. TFT seems to be the computationally heaviest out of those three NN models, while N-HiTS runs the quickest. For that reason, I will only include the Tuning for this model in my notebook.\n","\n","### COMPUTATION ISSUES\n","\n","Importantly, the following models are all trained with small subsets of the full timeseries (see the parameter *max_samples_per_ts* in the code) in order to make computation feasible. The Kaggle GPU is not quick enough to deal with bigger data I think. Therefore the following models should not be regarded as optimal at all, but rather serve as minimal examples.\n","\n","CatBoost is trained with the last 365 samples (input + output lengths) of each series, N-HiTS is trained with the last 180 samples, LSTM is trained with the last 60 samples, and TFT is trained with only the last 7 samples per series. I chose those numbers to roughly equalize the training time for each of those models after experimenting a bit."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.1\"></a> <br>\n","# 4.1. N-HiTS"]},{"cell_type":"markdown","metadata":{},"source":["\n","<img style=\"float: center;\" src=\"https://images.deepai.org/converted-papers/2201.12886/x4.png\">\n","\n","*source*: https://images.deepai.org/converted-papers/2201.12886/x4.png"]},{"cell_type":"markdown","metadata":{},"source":["N-HiTS only supports *past_covariates*. As I still want to use the future-known information on promotion, holidays and time dummies, I shift back those covariates 16 days to the past. I define a function for training the model and then create an Optuna study, which I let run for 5 trials on the relatively slow Kaggle GPU. That is not a lot and will likely not deliver very good hyperparameters, but suffices as an example."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T09:50:34.868716Z","iopub.status.busy":"2022-10-25T09:50:34.868101Z","iopub.status.idle":"2022-10-25T09:51:18.318021Z","shell.execute_reply":"2022-10-25T09:51:18.316865Z","shell.execute_reply.started":"2022-10-25T09:50:34.868672Z"},"trusted":true},"outputs":[],"source":["# Data Preparation for N-HiTS\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","# Shift future covariates back so they can be used as past covariates\n","\n","only_past_covariates = []\n","\n","for family in family_list:\n","  only_past_covariates.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates = flatten(only_past_covariates)\n","\n","NHiTS_covariates = []\n","\n","for i in range(0,len(future_covariates_full)):\n","  shifted = future_covariates_full[i].shift(n=-16)\n","  cut = shifted.slice_intersect(only_past_covariates[i])\n","  stacked = cut.stack(only_past_covariates[i])\n","  NHiTS_covariates.append(stacked)\n","    \n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","train = [s[: -(2 * val_len)] for s in training_transformed]\n","val = [s[-(2 * val_len) : -val_len] for s in training_transformed]\n","test = [s[-val_len:] for s in training_transformed]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T10:04:37.538065Z","iopub.status.busy":"2022-10-25T10:04:37.537703Z","iopub.status.idle":"2022-10-25T10:04:37.552442Z","shell.execute_reply":"2022-10-25T10:04:37.551403Z","shell.execute_reply.started":"2022-10-25T10:04:37.538032Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a N-HiTS Model, which we will re-use later.\n","\"\"\"\n","\n","def build_fit_nhits_model(\n","    input_chunk_length,\n","    num_stacks,\n","    num_blocks,\n","    num_layers,\n","    layer_exp,\n","    dropout,\n","    lr,\n","    likelihood=None,\n","    callbacks=None,\n","    #max_samples=None\n","):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_N_EPOCHS = 50\n","    MAX_SAMPLES_PER_TS = 180\n","\n","    # throughout training we'll monitor the validation loss for early stopping\n","    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n","    if callbacks is None:\n","        callbacks = [early_stopper]\n","    else:\n","        callbacks = [early_stopper] + callbacks\n","\n","    # detect if a GPU is available\n","    if torch.cuda.is_available():\n","        pl_trainer_kwargs = {\n","            \"accelerator\": \"gpu\",\n","            \"gpus\": 1,\n","            \"auto_select_gpus\": True,\n","            \"callbacks\": callbacks,\n","        }\n","    \n","        num_workers = 2\n","    else:\n","        pl_trainer_kwargs = {\"callbacks\": callbacks}\n","        num_workers = 0\n","\n","    # build the N-HiTS model\n","    model = NHiTSModel(\n","        input_chunk_length=input_chunk_length,\n","        output_chunk_length=16,\n","        num_stacks=num_stacks,\n","        num_blocks=num_blocks,\n","        num_layers=num_layers,\n","        layer_widths=2 ** layer_exp,\n","        dropout=dropout,\n","        n_epochs=MAX_N_EPOCHS,\n","        batch_size=128,\n","        add_encoders=None,\n","        likelihood=None, \n","        loss_fn=torch.nn.MSELoss(),\n","        random_state=42,\n","        optimizer_kwargs={\"lr\": lr},\n","        pl_trainer_kwargs=pl_trainer_kwargs,\n","        model_name=\"nhits_model\",\n","        force_reset=True,\n","        save_checkpoints=True,\n","    )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in training_transformed]\n","\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=NHiTS_covariates,\n","        val_past_covariates=NHiTS_covariates,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n","        num_loader_workers=num_workers,\n","    )\n","\n","    # reload best model over course of training\n","    model = NHiTSModel.load_from_checkpoint(\"nhits_model\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-25T10:04:45.32871Z","iopub.status.busy":"2022-10-25T10:04:45.328165Z","iopub.status.idle":"2022-10-25T10:07:58.219704Z","shell.execute_reply":"2022-10-25T10:07:58.216675Z","shell.execute_reply.started":"2022-10-25T10:04:45.328662Z"},"trusted":true},"outputs":[],"source":["# Hyperparameter Tuning with Optuna\n","\n","def objective(trial):\n","    callback = [PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")]\n","\n","    # set input_chunk_length, between 21 and 365 days\n","    input_chunk_length = trial.suggest_int(\"input_chunk_length\", 63, 270)\n","\n","    # Other hyperparameters\n","    num_stacks = trial.suggest_int(\"num_stacks\", 1, 3)\n","    num_blocks = trial.suggest_int(\"num_blocks\", 1, 3)\n","    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n","    layer_exp = trial.suggest_int(\"layer_exp\", 7, 10)\n","    #layer_widths = 2 ** layer_exp\n","    dropout = trial.suggest_float(\"dropout\", 0.01, 0.2, step=0.01)\n","    lr = trial.suggest_float(\"lr\", 5e-5, 0.1, log=True)\n","\n","    # build and train the N-HiTS model with these hyper-parameters:\n","    model = build_fit_nhits_model(\n","                        input_chunk_length=input_chunk_length,\n","                        num_stacks=num_stacks,\n","                          num_blocks=num_blocks,\n","                        num_layers=num_layers,\n","                        layer_exp=layer_exp,\n","                          dropout=dropout,\n","                              lr=lr,\n","                          likelihood=None,\n","                          callbacks=callback,\n","                          #max_samples=365\n","    )\n","\n","    # Evaluate how good it is on the validation set\n","    preds = model.predict(series=train, past_covariates=NHiTS_covariates, n=val_len)\n","    rmsles = rmsle(val, preds, n_jobs=-1, verbose=True)\n","    rmsle_val = np.mean(rmsles)\n","\n","    return rmsle_val if rmsle_val != np.nan else float(\"inf\")\n","\n","def print_callback(study, trial):\n","    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n","    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n","\n","\n","torch.cuda.empty_cache()\n","\n","study_nhits = optuna.create_study(direction=\"minimize\")\n","\n","study_nhits.optimize(objective, n_trials=5, callbacks=[print_callback])\n","\n","# Finally, print the best value and best hyperparameters:\n","print(f\"Best value: {study_nhits.best_value}, Best params: {study_nhits.best_trial.params}\")"]},{"cell_type":"markdown","metadata":{},"source":["So this are the best hyperparameters found:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["# Finally, print the best value and best hyperparameters:\n","print(f\"Best value: {study_nhits.best_value}, Best params: {study_nhits.best_trial.params}\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look at the Tuning process:"]},{"cell_type":"markdown","metadata":{},"source":["### Improvement of error score over Tuning trials"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["plot_optimization_history(study_nhits)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Importance over Tuning trials"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["plot_param_importances(study_nhits)"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Search space and corresponding error scores"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["plot_contour(study_nhits, params=[\"lr\", \"num_stacks\"])"]},{"cell_type":"markdown","metadata":{},"source":["Using the hyperparameters we got during this short Tuning session, we now train the N-HiTS model and make forecasts for the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["nhits_params = study_nhits.best_trial.params\n","\n","# get the start time\n","st = time.time()\n","\n","NHiTS_Model = build_fit_nhits_model(**nhits_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in training_transformed] \n","preds = NHiTS_Model.predict(series=training_data, past_covariates=NHiTS_covariates, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (list_of_TS[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","        \n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_nhits = et - st\n","\n","# Mean RMSLE\n","\n","NHiTS_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"The mean RMSLE for the Global N-HiTS Model over 1782 series is {:.5f}.\".format(NHiTS_rmsle))\n","print('Training & Inference duration:', elapsed_time_nhits, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.1\"></a> <br>\n","# 4.2. LSTM"]},{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://miro.medium.com/max/674/1*jikKbzFXCq-IYnFZankIMg.png\">\n","\n","*source*: https://miro.medium.com/max/674/1*jikKbzFXCq-IYnFZankIMg.png"]},{"cell_type":"markdown","metadata":{},"source":["LSTM only supports future-known covariates. In order to also utilize the data from our solely past-known covariates (sales and transactions data), I shift them forward 16 days into the future. While this approach might not be optimal, I found it to perform better than throwing out those covariates all-together.\n","\n","A little Fine-Tuning gave me the following hyperparameters for LSTM:\n","\n","* Input_chunk_length: 131 \n","* hidden_dim: 39 \n","* n_rnn_layers: 3\n","* learning rate: 0.0019971227090605087\n","\n","Let's train this model with all our data and make a forecast for the 16 days test set!"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-26T12:44:03.495898Z","iopub.status.busy":"2022-10-26T12:44:03.495428Z","iopub.status.idle":"2022-10-26T12:44:52.645094Z","shell.execute_reply":"2022-10-26T12:44:52.644079Z","shell.execute_reply.started":"2022-10-26T12:44:03.495838Z"},"trusted":true},"outputs":[],"source":["# Data Preparation for LSTM\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","# Shift past covariates forward so they can be used as future covariates\n","\n","\n","only_past_covariates = []\n","\n","for family in family_list:\n","  only_past_covariates.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates = flatten(only_past_covariates)\n","\n","\n","LSTM_covariates = []\n","\n","for i in range(0,len(only_past_covariates)):\n","  shifted = only_past_covariates[i].shift(n=16)\n","  cut = future_covariates_full[i].slice_intersect(shifted)\n","  stacked = cut.stack(shifted)\n","  LSTM_covariates.append(stacked)\n","    \n","# Slice-Intersect target and covariates after shifting\n","\n","LSTM_target = []\n","\n","for i in range(0, len(training_transformed)):\n","  sliced = training_transformed[i].slice_intersect(LSTM_covariates[i])\n","  LSTM_target.append(sliced)\n","\n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","LSTM_train = [s[: -(2 * val_len)] for s in LSTM_target]\n","LSTM_val = [s[-(2 * val_len) : -val_len] for s in LSTM_target]\n","LSTM_test = [s[-val_len:] for s in LSTM_target]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-26T12:45:00.49453Z","iopub.status.busy":"2022-10-26T12:45:00.494054Z","iopub.status.idle":"2022-10-26T12:45:00.507161Z","shell.execute_reply":"2022-10-26T12:45:00.505932Z","shell.execute_reply.started":"2022-10-26T12:45:00.49449Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n","\"\"\"\n","\n","def build_fit_lstm_model(\n","    input_chunk_length,\n","    hidden_dim,\n","    n_rnn_layers,\n","  #  dropout,\n","  #  training_length,\n","  #  batch_size,\n","  #  n_epochs,\n","    lr,\n","    likelihood=None,\n","    callbacks=None,\n","):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_N_EPOCHS = 100\n","    MAX_SAMPLES_PER_TS = 60\n","\n","    # throughout training we'll monitor the validation loss for early stopping\n","    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n","    if callbacks is None:\n","        callbacks = [early_stopper]\n","    else:\n","        callbacks = [early_stopper] + callbacks\n","\n","    # detect if a GPU is available\n","    if torch.cuda.is_available():\n","        pl_trainer_kwargs = {\n","            \"accelerator\": \"gpu\",\n","            \"gpus\": 1,\n","            \"auto_select_gpus\": True,\n","            \"callbacks\": callbacks,\n","        }\n","        num_workers = 2\n","    else:\n","        pl_trainer_kwargs = {\"callbacks\": callbacks}\n","        num_workers = 0\n","\n","    # build the LSTM model\n","    model = RNNModel(\n","        model=\"LSTM\",\n","        input_chunk_length=input_chunk_length,\n","        hidden_dim=hidden_dim,\n","        n_rnn_layers=n_rnn_layers,\n","        dropout=0,\n","        training_length=input_chunk_length + val_len -1,\n","        n_epochs=MAX_N_EPOCHS,\n","        batch_size=128,\n","        add_encoders=None,\n","        likelihood=None, \n","        loss_fn=torch.nn.MSELoss(),\n","        random_state=42,\n","        optimizer_kwargs={\"lr\": lr},\n","        pl_trainer_kwargs=pl_trainer_kwargs,\n","        model_name=\"lstm_model\",\n","        force_reset=True,\n","        save_checkpoints=True,\n","    )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in LSTM_target]\n","\n","\n","    # train the model\n","    model.fit(\n","        series=LSTM_train,\n","        val_series=model_val_set,\n","        future_covariates=LSTM_covariates,\n","        val_future_covariates=LSTM_covariates,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n","        num_loader_workers=num_workers,\n","    )\n","\n","    # reload best model over course of training\n","    model = RNNModel.load_from_checkpoint(\"lstm_model\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-26T12:45:05.155643Z","iopub.status.busy":"2022-10-26T12:45:05.154913Z","iopub.status.idle":"2022-10-26T12:46:00.732177Z","shell.execute_reply":"2022-10-26T12:46:00.722068Z","shell.execute_reply.started":"2022-10-26T12:45:05.155602Z"},"trusted":true},"outputs":[],"source":["lstm_params = {'input_chunk_length': 131, \n","               'hidden_dim': 39, \n","               'n_rnn_layers': 3, \n","               'lr': 0.0019971227090605087}\n","\n","torch.cuda.empty_cache()\n","\n","# get the start time\n","st = time.time()\n","\n","LSTM_Model = build_fit_lstm_model(**lstm_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in LSTM_target] \n","preds = LSTM_Model.predict(series=training_data, future_covariates=LSTM_covariates, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (LSTM_target[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","        \n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_lstm = et - st\n","\n","# Mean RMSLE\n","\n","LSTM_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"The mean RMSLE for the Global LSTM Model over 1782 series is {:.5f}.\".format(LSTM_rmsle))\n","print('Training & Inference duration:', elapsed_time_lstm, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.3\"></a> <br>\n","# 4.3. TFT"]},{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://miro.medium.com/max/1400/1*7rXe_MVn5QI9oLP2vrMdvQ.png\">\n","\n","*source*: https://miro.medium.com/max/1400/1*7rXe_MVn5QI9oLP2vrMdvQ.png"]},{"cell_type":"markdown","metadata":{},"source":["The TFT model natively supports all types of covariates including static ones. It is however computationally very heavy.\n","\n","I will use those hyperparameters:\n","\n","* input_chunk_length: 230\n","* output_chunk_length: 16 \n","* hidden_size: 16 \n","* lstm_layers: 3 \n","* num_attention_heads: 4\n","* full_attention: True\n","* hidden_continuous_size: 16\n","* dropout: 0.060000000000000005\n","* lr: 0.009912733600616069\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["# Data Preparation for TFT\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","only_past_covariates_full = []\n","\n","for family in family_list:\n","  only_past_covariates_full.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates_full = flatten(only_past_covariates_full)\n","\n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","train = [s[: -(2 * val_len)] for s in training_transformed]\n","val = [s[-(2 * val_len) : -val_len] for s in training_transformed]\n","test = [s[-val_len:] for s in training_transformed]"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n","\"\"\"\n","\n","\n","def build_fit_tft_model(\n","    input_chunk_length,\n","    output_chunk_length,\n","    hidden_size,\n","    lstm_layers,\n","    num_attention_heads,\n","    full_attention,\n","    #feed_forward,\n","    hidden_continuous_size,\n","    dropout,\n","    #batch_size,\n","    #add_relative_index,\n","    lr,\n","    likelihood=None,\n","    callbacks=None,\n","):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_N_EPOCHS = 100\n","    MAX_SAMPLES_PER_TS = 7\n","\n","    # throughout training we'll monitor the validation loss for early stopping\n","    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n","    if callbacks is None:\n","        callbacks = [early_stopper]\n","    else:\n","        callbacks = [early_stopper] + callbacks\n","\n","    # detect if a GPU is available\n","    if torch.cuda.is_available():\n","        pl_trainer_kwargs = {\n","            \"accelerator\": \"gpu\",\n","            \"gpus\": 1,\n","            \"auto_select_gpus\": True,\n","            \"callbacks\": callbacks,\n","        }\n","        num_workers = 2\n","    else:\n","        pl_trainer_kwargs = {\"callbacks\": callbacks}\n","        num_workers = 0\n","\n","    # build the TFT model\n","    model = TFTModel(\n","        input_chunk_length=input_chunk_length,\n","        output_chunk_length=output_chunk_length,\n","        hidden_size=hidden_size,\n","        lstm_layers=lstm_layers,\n","        num_attention_heads=num_attention_heads,\n","        full_attention =full_attention,\n","        #feed_forward = feed_forward,\n","        hidden_continuous_size = hidden_continuous_size,\n","        dropout=dropout,\n","        batch_size=128,\n","        n_epochs=MAX_N_EPOCHS,\n","        #add_relative_index=add_relative_index,\n","        add_encoders=None,\n","        likelihood=None, \n","        loss_fn=torch.nn.MSELoss(),\n","        random_state=42,\n","        optimizer_kwargs={\"lr\": lr},\n","        pl_trainer_kwargs=pl_trainer_kwargs,\n","        model_name=\"tft_model\",\n","        force_reset=True,\n","        save_checkpoints=True,\n","    )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in training_transformed]\n","\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_full,\n","        val_past_covariates=only_past_covariates_full,\n","        future_covariates=future_covariates_full,\n","        val_future_covariates=future_covariates_full,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n","        num_loader_workers=num_workers,\n","    )\n","\n","    # reload best model over course of training\n","    model = TFTModel.load_from_checkpoint(\"tft_model\")\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["tft_params = {'input_chunk_length': 230, \n","              'output_chunk_length': 16, \n","              'hidden_size': 16, \n","              'lstm_layers': 3, \n","              'num_attention_heads': 4, \n","              'full_attention': True, \n","              'hidden_continuous_size': 16, \n","              'dropout': 0.060000000000000005, \n","              'lr': 0.009912733600616069}\n","\n","torch.cuda.empty_cache()\n","\n","# get the start time\n","st = time.time()\n","\n","TFT_Model = build_fit_tft_model(**tft_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in training_transformed] \n","preds = TFT_Model.predict(series=training_data, past_covariates=only_past_covariates_full, future_covariates=future_covariates_full, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (list_of_TS[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_tft = et - st\n","\n","# Mean RMSLE\n","\n","TFT_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"The mean RMSLE for the Global TFT Model over 1782 series is {:.5f}.\".format(TFT_rmsle))\n","print('Training & Inference duration:', elapsed_time_tft, 'seconds')\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Boosted Trees"]},{"cell_type":"markdown","metadata":{},"source":["<img style=\"float: center;\" src=\"https://avatars.mds.yandex.net/get-yablogs/47421/file_1548410151831/orig\">\n","\n","*source*: https://avatars.mds.yandex.net/get-yablogs/47421/file_1548410151831/orig"]},{"cell_type":"markdown","metadata":{},"source":["Even though my focus has been on deep learning models, I actually found boosted tree models to perform best for this forecasting problem so far. Darts offers implementations of LightGBM and CatBoost. \n","\n","Due to out-of-memory issues in the Kaggle kernel, I commented out the training of boosted trees for now. From former notebooks and training on Colab however I can tell that those models have given the best scores so far."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true,"execution":{"iopub.execute_input":"2022-10-29T15:21:42.562685Z","iopub.status.busy":"2022-10-29T15:21:42.560299Z","iopub.status.idle":"2022-10-29T15:21:42.61505Z","shell.execute_reply":"2022-10-29T15:21:42.612495Z","shell.execute_reply.started":"2022-10-29T15:21:42.562495Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["\"\"\"\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","\n","future_covariates_full = []\n","\n","for family in family_list:\n","  future_covariates_full.append(future_covariates_dict[family])\n","    \n","future_covariates_full = flatten(future_covariates_full)\n","\n","\n","only_past_covariates_full = []\n","\n","for family in family_list:\n","  only_past_covariates_full.append(only_past_covariates_dict[family])\n","    \n","only_past_covariates_full = flatten(only_past_covariates_full)\n","\n","\n","only_past_covariates_shifted = []\n","\n","for ts in only_past_covariates_full:\n","  shifted = ts.shift(n=16)\n","  only_past_covariates_shifted.append(shifted)\n","\n","# Split in train/val/test for Tuning and Validation\n","\n","val_len = 16\n","\n","train = [s[: -(2 * val_len)] for s in training_transformed]\n","val = [s[-(2 * val_len) : -val_len] for s in training_transformed]\n","test = [s[-val_len:] for s in training_transformed]\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-29T14:58:47.443525Z","iopub.status.busy":"2022-10-29T14:58:47.443136Z","iopub.status.idle":"2022-10-29T14:58:47.45601Z","shell.execute_reply":"2022-10-29T14:58:47.454295Z","shell.execute_reply.started":"2022-10-29T14:58:47.443498Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a TCN Model, which we will re-use later.\n","\n","from darts.models import CatBoostModel\n","\n","def build_fit_cboost_model(\n","    lags,\n","    firstlag,\n","    pastcovlag,\n","    out_len,\n","    learning_rate,\n","    depth\n","    ):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # some fixed parameters that will be the same for all models\n","    MAX_SAMPLES_PER_TS = 365\n","\n","    # build the TCN model\n","    model = CatBoostModel(lags = lags,\n","                             lags_future_covariates = (firstlag,1),\n","                             lags_past_covariates = [-pastcovlag], \n","                             output_chunk_length=out_len,\n","                          learning_rate=learning_rate,\n","                          depth=depth,\n","                             early_stopping_rounds=10,\n","                             random_state=2022,\n","                          logging_level='Silent'\n","                          )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + lags) : -val_len] for s in training_transformed]\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_shifted,\n","        val_past_covariates=only_past_covariates_shifted,\n","        future_covariates=future_covariates_full,\n","        val_future_covariates=future_covariates_full,\n","        max_samples_per_ts=MAX_SAMPLES_PER_TS\n","    )\n","\n","    # reload best model over course of training\n","    #model = LightGBMModel.load_from_checkpoint(\"lgbm_model\")\n","\n","    return model\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-29T14:58:53.851026Z","iopub.status.busy":"2022-10-29T14:58:53.8506Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","catboost_params = {'lags': 144, \n","                   'out_len': 3, \n","                   'firstlag': 44, \n","                   'pastcovlag': 60, \n","                   'learning_rate': 0.06539829509538796, \n","                   'depth': 9}\n","\n","# get the start time\n","st = time.time()\n","\n","CatBoost_Model = build_fit_cboost_model(**catboost_params)\n","\n","# Generate Forecasts for the Test Data\n","training_data = [ts[:-16] for ts in training_transformed] \n","preds = CatBoost_Model.predict(series=training_data, past_covariates=only_past_covariates_shifted, future_covariates=future_covariates_full, n=val_len)\n","\n","# Transform Back\n","forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n","\n","# Zero Forecasting\n","for n in range(0,len(forecasts_back)):\n","  if (list_of_TS[n][:-16].univariate_values()[-14:] == 0).all():\n","        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n","\n","# get the end time\n","et = time.time()\n","\n","# get the execution time\n","elapsed_time_cboost = et - st\n","\n","# Mean RMSLE\n","\n","CatBoost_rmsle = rmsle(actual_series = list_of_TS,\n","                 pred_series = forecasts_back,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","print(\"\\n\")\n","print(\"The mean RMSLE for the Global CatBoost Model over 1782 series is {:.5f}.\".format(CatBoost_rmsle))\n","print('Training & Inference duration:', elapsed_time_cboost, 'seconds')\n","print(\"\\n\")\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-24T03:37:30.014346Z","iopub.status.busy":"2022-10-24T03:37:30.013185Z","iopub.status.idle":"2022-10-24T03:38:45.808688Z","shell.execute_reply":"2022-10-24T03:38:45.806979Z","shell.execute_reply.started":"2022-10-24T03:37:30.014307Z"},"trusted":true},"outputs":[],"source":["\"\"\" We write a function to build and fit a CatBoost Model, which we will re-use later.\n","\"\"\"\n","\"\"\"\n","from darts.models import CatBoostModel\n","\n","def build_fit_family_cboost_model(\n","    lags,\n","    firstlag,\n","    pastcovlag,\n","    out_len,\n","    ):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # build the CatBoost model\n","    model = CatBoostModel(lags = lags,\n","                             lags_future_covariates = (firstlag,1),\n","                             lags_past_covariates = [-pastcovlag], \n","                             output_chunk_length=out_len,\n","                             early_stopping_rounds=10,\n","                             random_state=2022\n","                          )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-((2 * val_len) + lags) : -val_len] for s in sales_family]\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_fam_shifted,\n","        val_past_covariates=only_past_covariates_fam_shifted,\n","        future_covariates=future_covariates_fam,\n","        val_future_covariates=future_covariates_fam\n","    )\n","\n","    return model\n","\n","# Train CatBoost Family Models\n","\n","CatBoost_Models = {}\n","\n","for family in tqdm(family_list):\n","    \n","  sales_family = family_TS_transformed_dict[family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","\n","  # Split in train/val/test\n","  val_len = 16\n","  train = [s[: -(2 * val_len)] for s in sales_family]\n","\n","  CatBoost_Model = build_fit_family_cboost_model( lags = 365,\n","    firstlag = 28,\n","    pastcovlag = 14,\n","    out_len = 1)\n","\n","  CatBoost_Models[family] = CatBoost_Model\n","  \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\"\n","# Generate Forecasts for the Test Data\n","\n","CatBoost_Forecasts_Families = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","\n","  forecast_CatBoost = CatBoost_Models[family].predict(n=16,\n","                                         series=training_data,\n","                                         future_covariates=future_covariates_fam,\n","                                         past_covariates=only_past_covariates_fam_shifted\n","                                         )\n","  \n","  CatBoost_Forecasts_Families[family] = forecast_CatBoost\n","\n","# Transform Back\n","\n","CatBoost_Forecasts_Families_back = {}\n","\n","for family in tqdm(family_list):\n","\n","  CatBoost_Forecasts_Families_back[family] = family_pipeline_dict[family].inverse_transform(CatBoost_Forecasts_Families[family], partial=True)\n","\n","# Zero Forecasting\n","\n","for family in tqdm(CatBoost_Forecasts_Families_back):\n","  for n in range(0,len(CatBoost_Forecasts_Families_back[family])):\n","    if (family_TS_dict[family][n][:-16].univariate_values()[-14:] == 0).all():\n","        CatBoost_Forecasts_Families_back[family][n] = CatBoost_Forecasts_Families_back[family][n].map(lambda x: x * 0)\n","        \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\"\n","# Re-Format all 1782 Forecasts in one List and Evaluate Performance\n","\n","forecast_list_CatBoost = []\n","\n","for family in family_list:\n","  forecast_list_CatBoost.append(CatBoost_Forecasts_Families_back[family])\n","\n","sales_data = []\n","\n","for family in family_list:\n","  sales_data.append(family_TS_dict[family])\n","\n","def flatten(l):\n","  return [item for sublist in l for item in sublist]\n","    \n","actual_list = flatten(sales_data)\n","pred_list_CatBoost = flatten(forecast_list_CatBoost)\n","\n","# Mean RMSLE\n","\n","CatBoost_rmsle = rmsle(actual_series = actual_list,\n","                 pred_series = pred_list_CatBoost,\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","\n","print(\"\\n\")\n","print(\"The mean RMSLE for the 33 CatBoost Global Product Family Models over all 1782 series is {:.5f}.\".format(CatBoost_rmsle))\n","print(\"\\n\")\n","\n","# Mean RMSLE for Families\n","\n","family_forecast_rmsle_CatBoost = {}\n","\n","for family in family_list:\n","\n","  CatBoost_rmsle_family = rmsle(actual_series = family_TS_dict[family],\n","                 pred_series = CatBoost_Forecasts_Families_back[family],\n","                 n_jobs = -1,\n","                 inter_reduction=np.mean)\n","  \n","  family_forecast_rmsle_CatBoost[family] = CatBoost_rmsle_family\n","\n","family_forecast_rmsle_CatBoost = dict(sorted(family_forecast_rmsle_CatBoost.items(), key=lambda item: item[1]))\n","\n","print(\"Mean RMSLE for the 33 different product families, from worst to best:\")\n","print(\"\\n\")\n","\n","# Iterate over key/value pairs in dict and print them\n","for key, value in family_forecast_rmsle_CatBoost.items():\n","    print(key, ' : ', value)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-20T22:26:00.336799Z","iopub.status.busy":"2022-09-20T22:26:00.336282Z","iopub.status.idle":"2022-09-20T22:26:39.414778Z","shell.execute_reply":"2022-09-20T22:26:39.413574Z","shell.execute_reply.started":"2022-09-20T22:26:00.336761Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Plot the five worst forecasts   \n","\n","errorlist = []\n","\n","for i in range(0, len(actual_list)):\n","\n","  error = rmsle(actual_series = actual_list[i], \n","                pred_series = pred_list_CatBoost[i])\n","  \n","  errorfam = actual_list[i].static_covariates_values()[0,1]\n","  \n","  errorlist.append([errorfam,error])\n","\n","rmsle_series_CatBoost = pd.DataFrame(errorlist,columns=['family','RMSLE'])\n","worst_3_CatBoost = rmsle_series_CatBoost.sort_values(by=['RMSLE'], ascending=False).head(3)\n","\n","for i in range(0, len(worst_3_CatBoost)):\n","  plt_forecast = pred_list_CatBoost[(worst_3_CatBoost.index[i])]\n","  plt_actual = actual_list[(worst_3_CatBoost.index[i])]\n","  plt_err = rmsle(plt_actual, plt_forecast)\n","\n","  plt.figure(figsize=(10, 6))\n","  plt_actual[-100:].plot(label=\"actual data\")\n","  plt_forecast.plot(label=\"CatBoost forecast\")\n","  plt.title(\"{} in store {} ({}) - RMSLE: {}\".format(plt_forecast.static_covariates_values()[0,1], \n","                                                plt_forecast.static_covariates_values()[0,0],\n","                                                plt_forecast.static_covariates_values()[0,2],\n","                                                plt_err))\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4.5\"></a> <br>\n","# 4.5. Model Comparison"]},{"cell_type":"markdown","metadata":{},"source":["Let's quickly compare the performance of the models we trained in this notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true},"outputs":[],"source":["print(\"\\n\")\n","print(\"Mean RMSLE for Local Exponential Smoothing Models: {:.5f}.\".format(ES_rmsle))\n","print('Training duration:', elapsed_time_exp, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"Mean RMSLE for Global N-HiTS Model: {:.5f}.\".format(NHiTS_rmsle))\n","print('Training duration:', elapsed_time_nhits, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"Mean RMSLE for Global LSTM Model: {:.5f}.\".format(LSTM_rmsle))\n","print('Training duration:', elapsed_time_lstm, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","print(\"Mean RMSLE for Global TFT Model: {:.5f}.\".format(TFT_rmsle))\n","print('Training duration:', elapsed_time_tft, 'seconds')\n","print(\"\\n\")\n","print(\"\\n\")\n","# print(\"Mean RMSLE for Global CatBoost Model{:.5f}.\".format(CatBoost_rmsle))\n","# print('Training duration:', elapsed_time_cboost, 'seconds')\n","# print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["But once again, those models are way from optimal, as they have not been trained with full data nor have their hyperparameters been tuned in a proper way, which requires a stronger machine (unless I coded the training process in a very inefficient way, then let me know please!).\n","\n","**Important:** Out of those global models, only TFT \"knows\" which series is which. It uses static covariates which contain info about store and product family, which identifies each series. N-HiTS, LSTM and LightGBM/CatBoost are trained with samples from all 1782 series without having direct information on the store/family ID of each series. That means, those models treat all samples as coming from the same data generating process. Whether that is a good assumption? Not sure - maybe it would work better to train global models for every product family or even store. There is always a trade-off between having more data and having more similarity between individual series for global models. I'm very interested in that topic - please comment if you have any ideas!"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"4\"></a> <br>\n","# 5. Submission on Leaderboard"]},{"cell_type":"markdown","metadata":{},"source":["Re-trained the baseline Exponential Smoothing models on full training data leads to a **public RMSLE Score of 0.40578** - that's good enough to scratch the Top 10%. 33 Global LightGBM Models (1 for each product family) achieved a #1 **Leaderboard Score of 0.38558** at the time of submission. Additionally I added the code for generating submission forecasts with CatBoost, but with one global CatBoost model per product family. Uncomment for using any of them."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-09-19T22:03:14.26878Z","iopub.status.busy":"2022-09-19T22:03:14.267963Z","iopub.status.idle":"2022-09-19T22:03:14.276192Z","shell.execute_reply":"2022-09-19T22:03:14.275046Z","shell.execute_reply.started":"2022-09-19T22:03:14.268734Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Train Final Exponential Smoothing Models and Forecast for Submission\n","\n","ES_Models_Family_Dict_Submission = {}\n","ES_Forecasts_Family_Dict_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts for ts in sales_family]\n","\n","  ES_Models_Family_Dict_Submission[family] = ESModelBuilder(training_data)\n","  forecasts_ES = ESForecaster(ES_Models_Family_Dict_Submission[family])\n","    \n","  # Transform Back\n","  ES_Forecasts_Family_Dict_Submission[family] = family_pipeline_dict[family].inverse_transform(forecasts_ES, partial=True)\n","\n","  # Zero Forecasting\n","  for i in range(0,len(ES_Forecasts_Family_Dict_Submission[family])):\n","      if (training_data[i].univariate_values()[-21:] == 0).all():\n","          ES_Forecasts_Family_Dict_Submission[family][i] = ES_Forecasts_Family_Dict_Submission[family][i].map(lambda x: x * 0)\n","    \n","    \n","# Prepare Submission in Correct Format\n","\n","listofseries = []\n","\n","for store in range(0,54):\n","  for family in tqdm(family_list):\n","      oneforecast = ES_Forecasts_Family_Dict_Submission[family][store].pd_dataframe()\n","      oneforecast.columns = ['fcast']\n","      listofseries.append(oneforecast)\n","\n","df_forecasts = pd.concat(listofseries) \n","df_forecasts.reset_index(drop=True, inplace=True)\n","\n","# No Negative Forecasts\n","df_forecasts[df_forecasts < 0] = 0\n","forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n","\n","# Submission\n","submission_kaggle = forecasts_kaggle_sorted\n","submission_kaggle.to_csv('submission.csv', index=False)\n","\n","# Train 33 Global LightGBM Models with Full Data\n","\n","from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\n","from lightgbm import early_stopping\n","\n","LGBM_Models_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  # Define Data for family\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts for ts in sales_family] \n","  TCN_covariates = future_covariates_dict[family]\n","  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n","\n","  LGBM_Model_Submission = LightGBMModel(lags = 63,\n","                             lags_future_covariates = (14,1),\n","                             lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],\n","                             output_chunk_length=1,\n","                             random_state=2022,\n","                         #    max_bin= [63],\n","                             gpu_use_dp= \"false\")\n","     \n","  LGBM_Model_Submission.fit(series=train_sliced, \n","                        future_covariates=TCN_covariates,\n","                        past_covariates=transactions_transformed,\n","                        verbose=True)\n","\n","  LGBM_Models_Submission[family] = LGBM_Model_Submission\n","  \n","  # Generate Forecasts for Submission\n","\n","LGBM_Forecasts_Families_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts for ts in sales_family]\n","  LGBM_covariates = future_covariates_dict[family]\n","  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n","\n","  forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n","                                         series=train_sliced,\n","                                         future_covariates=LGBM_covariates,\n","                                         past_covariates=transactions_transformed)\n","  \n","  LGBM_Forecasts_Families_Submission[family] = forecast_LGBM\n","\n","# Transform Back\n","\n","LGBM_Forecasts_Families_back_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)\n","\n","# Zero Forecasting\n","\n","for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n","  for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n","    if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n","        LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n","        \n","# Prepare Submission in Correct Format\n","\n","listofseries = []\n","\n","for store in range(0,54):\n","  for family in tqdm(family_list):\n","      oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n","      oneforecast.columns = ['fcast']\n","      listofseries.append(oneforecast)\n","\n","df_forecasts = pd.concat(listofseries) \n","df_forecasts.reset_index(drop=True, inplace=True)\n","\n","# No Negative Forecasts\n","df_forecasts[df_forecasts < 0] = 0\n","forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n","\n","# Submission\n","submission_kaggle = forecasts_kaggle_sorted\n","submission_kaggle.to_csv('submission.csv', index=False)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"source":["\"\"\" \n","from darts.models import CatBoostModel\n","\n","def build_fit_family_cboost_model(\n","    lags,\n","    firstlag,\n","    pastcovlag,\n","    out_len,\n","    ):\n","\n","    # reproducibility\n","    torch.manual_seed(42)\n","\n","    # build the CatBoost model\n","    model = CatBoostModel(lags = lags,\n","                             lags_future_covariates = (firstlag,1),\n","                             lags_past_covariates = [-pastcovlag], \n","                             output_chunk_length=out_len,\n","                             learning_rate=learning_rate,\n","                             depth=depth,\n","                             early_stopping_rounds=10,\n","                             random_state=2022\n","                          )\n","\n","    # when validating during training, we can use a slightly longer validation\n","    # set which also contains the first input_chunk_length time steps\n","    model_val_set = [s[-(val_len + lags) : ] for s in sales_family]\n","\n","    # train the model\n","    model.fit(\n","        series=train,\n","        val_series=model_val_set,\n","        past_covariates=only_past_covariates_fam_shifted,\n","        val_past_covariates=only_past_covariates_fam_shifted,\n","        future_covariates=future_covariates_fam,\n","        val_future_covariates=future_covariates_fam\n","    )\n","\n","    return model\n","    \n","# Train Submission Model\n","\n","CatBoost_Models = {}\n","\n","for family in tqdm(family_list):\n","    \n","  sales_family = family_TS_transformed_dict[family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","\n","  # Split in train/val/test\n","  val_len = 16\n","  train = [s[: -val_len] for s in sales_family]\n","\n","  CatBoost_Model = build_fit_family_cboost_model( lags = 144,\n","    firstlag = 44,\n","    pastcovlag = 60,\n","    learning_rate = 0.06539829509538796,\n","    depth = 9,\n","    out_len = 3)\n","\n","  CatBoost_Models[family] = CatBoost_Model\n","  \n","# Generate Forecasts for Submission\n","\n","CatBoost_Forecasts_Families_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  sales_family = family_TS_transformed_dict[family]\n","  training_data = [ts[:-16] for ts in sales_family]\n","  future_covariates_fam = future_covariates_dict[family]\n","  only_past_covariates_fam = only_past_covariates_dict[family]\n","                                                       \n","  only_past_covariates_fam_shifted = []                                                      \n","  for ts in only_past_covariates_fam:\n","      shifted = ts.shift(n=16)\n","      only_past_covariates_fam_shifted.append(shifted)\n","                                                       \n","  forecast_CatBoost = CatBoost_Models[family].predict(n=16,\n","                                         series=training_data,\n","                                         future_covariates=future_covariates_fam,\n","                                         past_covariates=only_past_covariates_fam_shifted)\n","  \n","  CatBoost_Forecasts_Families_Submission[family] = forecast_CatBoost\n","\n","# Transform Back\n","\n","CatBoost_Forecasts_Families_back_Submission = {}\n","\n","for family in tqdm(family_list):\n","\n","  CatBoost_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(CatBoost_Forecasts_Families_Submission[family], partial=True)\n","\n","# Zero Forecasting\n","\n","for family in tqdm(CatBoost_Forecasts_Families_back_Submission):\n","  for n in range(0,len(CatBoost_Forecasts_Families_back_Submission[family])):\n","    if (family_TS_dict[family][n].univariate_values()[-14:] == 0).all():\n","        CatBoost_Forecasts_Families_back_Submission[family][n] = CatBoost_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n","        \n","# Prepare Submission in Correct Format\n","\n","listofseries = []\n","\n","for store in range(0,54):\n","  for family in tqdm(family_list):\n","      oneforecast = CatBoost_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n","      oneforecast.columns = ['fcast']\n","      listofseries.append(oneforecast)\n","\n","df_forecasts = pd.concat(listofseries) \n","df_forecasts.reset_index(drop=True, inplace=True)\n","\n","# No Negative Forecasts\n","df_forecasts[df_forecasts < 0] = 0\n","forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n","forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n","\n","# Submission\n","submission_kaggle = forecasts_kaggle_sorted\n","submission_kaggle.to_csv('submission.csv', index=False)\n","\n","\"\"\""]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":2887556,"sourceId":29781,"sourceType":"competition"}],"dockerImageVersionId":30236,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"myenv","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
